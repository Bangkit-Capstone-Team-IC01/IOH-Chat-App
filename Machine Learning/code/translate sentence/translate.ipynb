{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translate",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AxhXmDOQApMo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense,AdditiveAttention,StringLookup,TextVectorization\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open(\"/content/drive/MyDrive/data/eng-ind.csv\") as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  next(csv_reader)\n",
        "\n",
        "  input_data = list()\n",
        "  target_data = list()\n",
        "\n",
        "  for row in csv_reader:\n",
        "    input_data.append(row[0])\n",
        "    target_data.append(row[1])"
      ],
      "metadata": {
        "id": "4ID8ZvSWBDvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fc3ccf-069f-4053-fbdf-9d0d8e459b73"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_data)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "7QgEbE8DEeFG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in dataset.take(1):\n",
        "  print(input_batch,\"\\n\")\n",
        "  print(target_batch)\n",
        "  break"
      ],
      "metadata": {
        "id": "NYgVVwDuIvL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "aQhNbdwIQHGU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor = TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "metadata": {
        "id": "_FtgXEKxPjrj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_processor.adapt(input_data)\n",
        "output_text_processor.adapt(target_data)"
      ],
      "metadata": {
        "id": "iuAIbJj1Q8M3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_input = 5000\n",
        "vocab_size_output = 5000\n",
        "embedding_dim = 512\n",
        "units = 64"
      ],
      "metadata": {
        "id": "N4rVODO2JC9d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "metadata": {
        "id": "z9oed6j4LFbw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(64,))\n",
        "# encoding network\n",
        "embedding_enc = Embedding(vocab_size_input, embedding_dim)(inputs)\n",
        "output_enc, state_enc = GRU(\n",
        "    units,return_sequences=True,\n",
        "    return_state=True,\n",
        "    recurrent_initializer='glorot_uniform')(embedding_enc)\n",
        "\n",
        "# attention network (units, query, value, mask)\n",
        "\n",
        "# decoding network embedding, gru, bahdanauattention, 2dense\n",
        "embedding_dec = Embedding(vocab_size_output, embedding_dim)(state_enc)\n",
        "rnn_output, state_dec = GRU(units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')(embedding_dec)\n",
        "# attentionnya masukin\n",
        "x1 = Dense(units, use_bias=False)(rnn_output)\n",
        "x2 = Dense(units, use_bias=False)(output_enc)\n",
        "\n",
        "query_mask = tf.ones(tf.shape(rnn_output)[:-1], dtype=bool)\n",
        "value_mask = (inputs != 0)\n",
        "\n",
        "context_vector, attention_weights = AdditiveAttention()(\n",
        "                      inputs = [x1,output_enc,x2],\n",
        "                      mask= [query_mask, value_mask],\n",
        "                      return_attention_scores = True,\n",
        "                  )\n",
        "\n",
        "context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "\n",
        "x3 = Dense(units, activation=tf.math.tanh,use_bias=False)(context_and_rnn_output)\n",
        "x4 = Dense(vocab_size_output)(x3)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=x4)\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=SparseCategoricalCrossentropy,\n",
        "    metrics = [batch_loss]\n",
        ")\n"
      ],
      "metadata": {
        "id": "X_65I-SVRoTL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "vi3APozWR9ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a5b3cd-f502-4bf3-90f0-8a917a0fe2b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 64)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 64, 512)      2560000     ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " gru_6 (GRU)                    [(None, 64, 64),     110976      ['embedding_6[0][0]']            \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, 64, 512)      2560000     ['gru_6[0][1]']                  \n",
            "                                                                                                  \n",
            " gru_7 (GRU)                    [(None, 64, 64),     110976      ['embedding_7[0][0]']            \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " tf.compat.v1.shape_3 (TFOpLamb  (3,)                0           ['gru_7[0][0]']                  \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem_6 (Sl  (2,)                0           ['tf.compat.v1.shape_3[0][0]']   \n",
            " icingOpLambda)                                                                                   \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 64, 64)       4096        ['gru_7[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 64, 64)       4096        ['gru_6[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.ones_3 (TFOpLambda)         (None, 64)           0           ['tf.__operators__.getitem_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " tf.__operators__.ne_3 (TFOpLam  (None, 64)          0           ['input_4[0][0]']                \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " additive_attention_3 (Additive  ((None, 64, 64),    64          ['dense_12[0][0]',               \n",
            " Attention)                      (None, 64, 64))                  'gru_6[0][0]',                  \n",
            "                                                                  'dense_13[0][0]',               \n",
            "                                                                  'tf.ones_3[0][0]',              \n",
            "                                                                  'tf.__operators__.ne_3[0][0]']  \n",
            "                                                                                                  \n",
            " tf.concat_3 (TFOpLambda)       (None, 64, 128)      0           ['additive_attention_3[0][0]',   \n",
            "                                                                  'gru_7[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 64, 64)       8192        ['tf.concat_3[0][0]']            \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 64, 5000)     325000      ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,683,400\n",
            "Trainable params: 5,683,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}