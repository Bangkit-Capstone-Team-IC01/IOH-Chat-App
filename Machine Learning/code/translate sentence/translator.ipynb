{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator amin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoPI-CW88EcR",
        "outputId": "bfe21155-7815-462e-eab8-3462dd7a0ca4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (14.0.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.46.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.25.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "56aTseK4q9ol"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git_dir = \"/content/IOH-Chat-App\"\n",
        "git_url = \"https://github.com/Bangkit-Capstone-Team/IOH-Chat-App.git\"\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call([\"git\", \"clone\", git_url])"
      ],
      "metadata": {
        "id": "2ZtMplOQv6Lz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filedir):\n",
        "  df = pd.read_csv(filedir)\n",
        "\n",
        "  return df.English.tolist(), df.Indonesia.values.tolist()"
      ],
      "metadata": {
        "id": "tRYdCkfh6Bhw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/IOH-Chat-App/Machine Learning/datasets/translate sentence/result/eng-ind.csv\"\n",
        "\n",
        "eng_sentences, ind_sentences = load_data(dataset_dir)\n",
        "\n",
        "print(f\"Num of english sentence: {len(eng_sentences)}\")\n",
        "print(f\"Num of indonesia sentence: {len(ind_sentences)}\")\n",
        "print()\n",
        "print(f\"English example: {eng_sentences[-1]}\")\n",
        "print(f\"Indonesia example: {ind_sentences[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW7fD1GdrExy",
        "outputId": "fa2cca35-b10c-4e51-e917-32302c630f72"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of english sentence: 8819\n",
            "Num of indonesia sentence: 8819\n",
            "\n",
            "English example: If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\n",
            "Indonesia example: Jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa, maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_preprocess(text):\n",
        "  text = tf_text.normalize_utf8(text).numpy().decode()\n",
        "  text = text.lower().strip()\n",
        "  text = text.replace(\"\\t\\n\", \"\")\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "_CM74wxj8uxy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentences = np.array(list(map(normalize_and_preprocess, eng_sentences)))\n",
        "ind_sentences = np.array(list(map(normalize_and_preprocess, ind_sentences)))"
      ],
      "metadata": {
        "id": "ERT9afZuHydF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(sentence, max_vocab):\n",
        "  tokenizer = Tokenizer(num_words=max_vocab)\n",
        "  tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "dIHbMKGvrHnD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seqs(tokenizer, maxlen=None):\n",
        "  return pad_sequences(tokenizer, maxlen=maxlen, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "ThNTAXptY_0A"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab = 8000\n",
        "\n",
        "eng_tokenizer = tokenizer(eng_sentences, max_vocab)\n",
        "ind_tokenizer = tokenizer(ind_sentences, max_vocab)\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "ind_tokenizer.fit_on_texts(ind_sentences)\n",
        "\n",
        "eng_encode_example = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "ind_encode_example = ind_tokenizer.texts_to_sequences(ind_sentences)\n",
        "\n",
        "eng_vocab = eng_tokenizer.index_word\n",
        "ind_vocab = ind_tokenizer.index_word\n",
        "\n",
        "eng_decode_example = eng_tokenizer.sequences_to_texts(eng_encode_example)\n",
        "ind_decode_example = ind_tokenizer.sequences_to_texts(ind_encode_example)\n",
        "\n",
        "eng_maxlen = max([len(i)for i in eng_decode_example])\n",
        "ind_maxlen = max([len(i)for i in ind_decode_example])\n",
        "\n",
        "print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "print(f\"English sequences: {eng_encode_example[-1]}\")\n",
        "print()\n",
        "print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "print(f\"Indonesia sequences: {ind_encode_example[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLMndXAX1zQD",
        "outputId": "f736f31c-33b0-4ebc-f709-48e5c552654b"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence: if a person has not had a chance to acquire his target language by the time he's an adult he's unlikely to be able to reach native speaker level in that language\n",
            "English sequences: [70, 7, 448, 42, 28, 63, 7, 692, 5, 4087, 32, 4088, 660, 78, 3, 43, 127, 77, 4089, 127, 2249, 5, 25, 258, 5, 1290, 1641, 1623, 4090, 15, 11, 660]\n",
            "\n",
            "Indonesia sentence: jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut\n",
            "Indonesia sequences: [119, 290, 3, 4869, 17, 1607, 71, 4, 4870, 158, 4871, 1165, 823, 242, 534, 80, 12, 15, 1719, 4872, 4873, 1160, 46, 71, 248]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "buffer_size = len(eng_sentences)\n",
        "\n",
        "eng_pad_seqs = tf.convert_to_tensor(pad_seqs(eng_encode_example, maxlen=eng_maxlen))\n",
        "ind_pad_seqs = tf.convert_to_tensor(pad_seqs(ind_encode_example, maxlen=eng_maxlen))\n",
        "\n",
        "eng_pad_seqs = tf.data.Dataset.from_tensor_slices((eng_pad_seqs, ind_pad_seqs)).shuffle(buffer_size).batch(batch_size)\n",
        "\n",
        "# eng_pad_seqs = tf.reshape(eng_pad_seqs, (*eng_pad_seqs.shape, 1))\n",
        "# ind_pad_seqs = tf.reshape(ind_pad_seqs, (*eng_pad_seqs.shape[:-1], 1))\n",
        "\n",
        "for input, target in dataset.take(1):\n",
        "  print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "  print(f\"English sequences: {input}\")\n",
        "  print()\n",
        "  print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "  print(f\"Indonesia sequences: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P9pIKvf-0U0",
        "outputId": "85855734-798f-4f13-e0b8-83009039c49a"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence: if a person has not had a chance to acquire his target language by the time he's an adult he's unlikely to be able to reach native speaker level in that language\n",
            "English sequences: [[ 70 266   7 ...   0   0   0]\n",
            " [  2  30   5 ...   0   0   0]\n",
            " [215   5   3 ...   0   0   0]\n",
            " ...\n",
            " [ 53 135 117 ...   0   0   0]\n",
            " [110 442 727 ...   0   0   0]\n",
            " [ 41   6   7 ...   0   0   0]]\n",
            "\n",
            "Indonesia sentence: jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut\n",
            "Indonesia sequences: [[119  19 880 ...   0   0   0]\n",
            " [  1  21 168 ...   0   0   0]\n",
            " [747  16 958 ...   0   0   0]\n",
            " ...\n",
            " [ 63 112   4 ...   0   0   0]\n",
            " [734   2 491 ...   0   0   0]\n",
            " [ 11  57   4 ...   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dims = 256\n",
        "epochs = 5\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "       from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_len, output_len, embed_dims, maxlen):\n",
        "  # # Model Architecure 1\n",
        "  # en_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  # en_embedding = layers.Embedding(input_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  # en_lstm = layers.LSTM(512, return_state=True)\n",
        "  # en_outputs, state_h, state_c = en_lstm(en_inputs)\n",
        "  # en_states = [state_h, state_c]\n",
        "\n",
        "  # dec_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  # dec_embedding = layers.Embedding(output_len, embed_dims)(dec_inputs)\n",
        "  \n",
        "  # dec_lstm = layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "  # dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)\n",
        "  # x = layers.Dense(1024, activation=tf.nn.relu)(dec_outputs)\n",
        "  # x = layers.Dropout(.5)(x)\n",
        "  # outputs = layers.Dense(output_len, activation=tf.nn.softmax)(x)\n",
        "\n",
        "  # model = Model([en_inputs, dec_inputs], outputs)\n",
        "\n",
        "  # Model Architecure 2\n",
        "  en_inputs = layers.Input(shape=(maxlen,))\n",
        "  en_embedding = layers.Embedding(input_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  en_gru = layers.GRU(512, return_sequences=True, return_state=True,)\n",
        "  en_outputs, en_state = en_gru(en_embedding)\n",
        "\n",
        "  dec_embedding = layers.Embedding(output_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  dec_gru = layers.GRU(512, return_sequences=True, return_state=True)\n",
        "  dec_outputs, dec_state = dec_gru(dec_embedding, initial_state=en_state)\n",
        "\n",
        "  x = layers.Dense(512, activation=tf.nn.relu)(dec_outputs)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  x = layers.Dense(1024, activation=tf.nn.relu)(dec_outputs)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  outputs = layers.Dense(output_len, activation=tf.nn.softmax)(x)\n",
        "\n",
        "  model = Model(en_inputs, outputs)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EMWZXYLLwiwP"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/training_checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq=10,\n",
        "    verbose=1, \n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    len(eng_vocab) + 1, \n",
        "    len(ind_vocab) + 1, \n",
        "    embed_dims,\n",
        "    eng_maxlen,\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.save_weights(checkpoint_path.format(epoch=0))"
      ],
      "metadata": {
        "id": "2cqjvZ5Owjsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1082227b-4136-42db-a5c2-178ccc8d6876"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)          [(None, 161)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_60 (Embedding)       (None, 161, 256)     1047296     ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_61 (Embedding)       (None, 161, 256)     1247744     ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " gru_56 (GRU)                   [(None, 161, 512),   1182720     ['embedding_60[0][0]']           \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " gru_57 (GRU)                   [(None, 161, 512),   1182720     ['embedding_61[0][0]',           \n",
            "                                 (None, 512)]                     'gru_56[0][1]']                 \n",
            "                                                                                                  \n",
            " dense_67 (Dense)               (None, 161, 1024)    525312      ['gru_57[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 161, 1024)    0           ['dense_67[0][0]']               \n",
            "                                                                                                  \n",
            " dense_68 (Dense)               (None, 161, 4874)    4995850     ['dropout_29[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,181,642\n",
            "Trainable params: 10,181,642\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset,\n",
        "          epochs=epochs,\n",
        "          callbacks=[cp_callback],\n",
        "          verbose=1)"
      ],
      "metadata": {
        "id": "HJygvxEGlUWz",
        "outputId": "5bc9e594-ed42-4fe0-f53d-0c288067bd25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "  9/138 [>.............................] - ETA: 27:52 - loss: 8.1886 - accuracy: 0.8597WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 19/138 [===>..........................] - ETA: 25:25 - loss: 7.8396 - accuracy: 0.9159WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 29/138 [=====>........................] - ETA: 23:17 - loss: 7.7313 - accuracy: 0.9334WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 39/138 [=======>......................] - ETA: 21:04 - loss: 7.6785 - accuracy: 0.9419WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 49/138 [=========>....................] - ETA: 18:56 - loss: 7.6472 - accuracy: 0.9470WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 59/138 [===========>..................] - ETA: 16:54 - loss: 7.6264 - accuracy: 0.9505WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 69/138 [==============>...............] - ETA: 14:48 - loss: 7.6118 - accuracy: 0.9528WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 79/138 [================>.............] - ETA: 12:40 - loss: 7.6007 - accuracy: 0.9547WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 89/138 [==================>...........] - ETA: 10:32 - loss: 7.5923 - accuracy: 0.9560WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 99/138 [====================>.........] - ETA: 8:23 - loss: 7.5854 - accuracy: 0.9572WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "109/138 [======================>.......] - ETA: 6:16 - loss: 7.5798 - accuracy: 0.9582WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "119/138 [========================>.....] - ETA: 4:07 - loss: 7.5752 - accuracy: 0.9589WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "129/138 [===========================>..] - ETA: 1:57 - loss: 7.5712 - accuracy: 0.9596WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "138/138 [==============================] - 1801s 13s/step - loss: 7.5683 - accuracy: 0.9601\n",
            "Epoch 2/5\n",
            "  1/138 [..............................] - ETA: 29:09 - loss: 7.5242 - accuracy: 0.9678WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 11/138 [=>............................] - ETA: 26:43 - loss: 7.5252 - accuracy: 0.9668WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 21/138 [===>..........................] - ETA: 24:34 - loss: 7.5253 - accuracy: 0.9668WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 31/138 [=====>........................] - ETA: 22:35 - loss: 7.5253 - accuracy: 0.9667WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 41/138 [=======>......................] - ETA: 20:37 - loss: 7.5252 - accuracy: 0.9669WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 51/138 [==========>...................] - ETA: 18:35 - loss: 7.5251 - accuracy: 0.9669WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 61/138 [============>.................] - ETA: 16:30 - loss: 7.5250 - accuracy: 0.9670WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 71/138 [==============>...............] - ETA: 14:23 - loss: 7.5249 - accuracy: 0.9671WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 81/138 [================>.............] - ETA: 12:14 - loss: 7.5248 - accuracy: 0.9672WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 91/138 [==================>...........] - ETA: 10:05 - loss: 7.5248 - accuracy: 0.9673WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "101/138 [====================>.........] - ETA: 7:56 - loss: 7.5247 - accuracy: 0.9673WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "111/138 [=======================>......] - ETA: 5:46 - loss: 7.5248 - accuracy: 0.9673WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "114/138 [=======================>......] - ETA: 5:08 - loss: 7.5248 - accuracy: 0.9673"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/saved_model/model.h5\"\n",
        "saved_model_dir = os.path.dirname(saved_model_path)\n",
        "\n",
        "if os.path.exists(saved_model_dir):\n",
        "  shutil.rmtree(saved_model_dir)\n",
        "\n",
        "model.save(saved_model_path)"
      ],
      "metadata": {
        "id": "0QGxWYuiQPnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "metadata": {
        "id": "fXrqo0owQrVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "  en_outputs, en_state = model.layers[2].output\n",
        "  en_model = Model(model.input[0], en_state)\n",
        "\n",
        "  dec_state = layers.Input(shape=(512,))\n",
        "  dec_inputs = dec_state\n",
        "\n",
        "  dec_gru = model.layers[3]\n",
        "  dec_outputs, dec_state= dec_gru(model.input[0], initial_state=dec_inputs)\n",
        "  \n",
        "  dec_dense1 = model.layers[4](dec_outputs)\n",
        "  dec_dropout1 = model.layers[5](dec_dense1)\n",
        "  dec_dense2 = model.layers[6](dec_dropout1)\n",
        "  dec_dropout2 = model.layers[7](dec_dense2)\n",
        "\n",
        "  dec_model = Model(model.input[0] + dec_inputs, \n",
        "                    [dec_dropout2] + dec_state)\n",
        "  \n",
        "  return en_model, dec_model"
      ],
      "metadata": {
        "id": "kcJQFJJrhTzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_model, dec_model = load_model(saved_model_path)"
      ],
      "metadata": {
        "id": "rh4i5L5hEgKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "  tokens = list()\n",
        "\n",
        "  sequences = eng_tokenizer.texts_to_sequences([text])\n",
        "  sequences = tf.convert_to_tensor(pad_seqs(sequences))\n",
        "\n",
        "  input = en_model.predict(sequences)\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  \n",
        "  for i in sequences:\n",
        "    output_chars, h, c = dec_model.predict([target_seq] + input)\n",
        "    char_index = np.argmax(output_chars)\n",
        "    text_char = ind_tokenizer.index_word[char_index]\n",
        "    tokens.append(text_char)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = char_index\n",
        "    states_value = [h, c]\n",
        "  \n",
        "  sentence = \" \".join(tokens)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "vY1J-UH1kfmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"if a person has not had a chance to acquire his target language by the time\")"
      ],
      "metadata": {
        "id": "PcvPTORhE5ph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}