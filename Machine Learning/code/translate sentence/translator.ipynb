{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator amin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow_text"
      ],
      "metadata": {
        "id": "VoPI-CW88EcR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "56aTseK4q9ol"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GIT_DIR = \"/content/IOH-Chat-App\"\n",
        "GIT_URL = \"https://github.com/Bangkit-Capstone-Team/IOH-Chat-App.git\"\n",
        "\n",
        "if not os.path.exists(GIT_DIR):\n",
        "  subprocess.call([\"git\", \"clone\", GIT_URL])"
      ],
      "metadata": {
        "id": "2ZtMplOQv6Lz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filedir):\n",
        "  df = pd.read_csv(filedir)\n",
        "\n",
        "  return df.English.tolist(), df.Indonesia.values.tolist()"
      ],
      "metadata": {
        "id": "tRYdCkfh6Bhw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = \"/content/IOH-Chat-App/Machine Learning/datasets/translate sentence/result/eng-ind.csv\"\n",
        "\n",
        "eng_sentences, ind_sentences = load_data(DATASET_DIR)\n",
        "\n",
        "print(f\"Num of english sentence: {len(eng_sentences)}\")\n",
        "print(f\"Num of indonesia sentence: {len(ind_sentences)}\")\n",
        "print()\n",
        "print(f\"English example: {eng_sentences[-1]}\")\n",
        "print(f\"Indonesia example: {ind_sentences[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW7fD1GdrExy",
        "outputId": "4b73bfc8-fb01-4e49-9788-4b804af7340d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of english sentence: 8819\n",
            "Num of indonesia sentence: 8819\n",
            "\n",
            "English example: If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\n",
            "Indonesia example: Jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa, maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_preprocess(text):\n",
        "  text = tf_text.normalize_utf8(text).numpy().decode()\n",
        "  text = text.lower().strip()\n",
        "  text = text.replace(\"\\t\\n\", \"\")\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "_CM74wxj8uxy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentences = np.array(list(map(normalize_and_preprocess, eng_sentences)))\n",
        "ind_sentences = np.array(list(map(normalize_and_preprocess, ind_sentences)))"
      ],
      "metadata": {
        "id": "ERT9afZuHydF",
        "outputId": "0fd82985-bf56-4c7d-9f6f-df8f7bbd03f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(sentence, max_vocab):\n",
        "  tokenizer = Tokenizer(num_words=max_vocab)\n",
        "  tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "dIHbMKGvrHnD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seqs(tokenizer, maxlen=None):\n",
        "  return pad_sequences(tokenizer, maxlen=maxlen, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "ThNTAXptY_0A"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab = 8000\n",
        "\n",
        "eng_tokenizer = tokenizer(eng_sentences, max_vocab)\n",
        "ind_tokenizer = tokenizer(ind_sentences, max_vocab)\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "ind_tokenizer.fit_on_texts(ind_sentences)\n",
        "\n",
        "eng_encode_example = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "ind_encode_example = ind_tokenizer.texts_to_sequences(ind_sentences)\n",
        "\n",
        "eng_decode_example = eng_tokenizer.sequences_to_texts(eng_encode_example)\n",
        "ind_decode_example = ind_tokenizer.sequences_to_texts(ind_encode_example)\n",
        "\n",
        "maxlen = max([len(i)for i in eng_decode_example])\n",
        "\n",
        "print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "print(f\"English sequences: {eng_encode_example[-1]}\")\n",
        "print()\n",
        "print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "print(f\"Indonesia sequences: {ind_encode_example[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLMndXAX1zQD",
        "outputId": "3026238a-d3d3-4fb2-b08c-992d0bc29433"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence: if a person has not had a chance to acquire his target language by the time he's an adult he's unlikely to be able to reach native speaker level in that language\n",
            "English sequences: [70, 7, 448, 42, 28, 63, 7, 692, 5, 4087, 32, 4088, 660, 78, 3, 43, 127, 77, 4089, 127, 2249, 5, 25, 258, 5, 1290, 1641, 1623, 4090, 15, 11, 660]\n",
            "\n",
            "Indonesia sentence: jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut\n",
            "Indonesia sequences: [119, 290, 3, 4869, 17, 1607, 71, 4, 4870, 158, 4871, 1165, 823, 242, 534, 80, 12, 15, 1719, 4872, 4873, 1160, 46, 71, 248]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pad_seqs = tf.convert_to_tensor(pad_seqs(eng_encode_example, maxlen=maxlen))\n",
        "ind_pad_seqs = tf.convert_to_tensor(pad_seqs(ind_encode_example, maxlen=maxlen))\n",
        "\n",
        "print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "print(f\"English sequences: {eng_pad_seqs[-1]}\")\n",
        "print()\n",
        "print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "print(f\"Indonesia sequences: {ind_pad_seqs[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P9pIKvf-0U0",
        "outputId": "2d65dfea-8eb6-45d9-bb99-b641fe969168"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence: if a person has not had a chance to acquire his target language by the time he's an adult he's unlikely to be able to reach native speaker level in that language\n",
            "English sequences: [  70    7  448   42   28   63    7  692    5 4087   32 4088  660   78\n",
            "    3   43  127   77 4089  127 2249    5   25  258    5 1290 1641 1623\n",
            " 4090   15   11  660    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n",
            "\n",
            "Indonesia sentence: jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut\n",
            "Indonesia sequences: [ 119  290    3 4869   17 1607   71    4 4870  158 4871 1165  823  242\n",
            "  534   80   12   15 1719 4872 4873 1160   46   71  248    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_DIMS = 256\n",
        "LR = 1e-4\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "LOSS = tf.keras.losses.sparse_categorical_crossentropy"
      ],
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_len, output_len, embed_dims, input_length):\n",
        "\n",
        "  model=tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Embedding(input_len, embed_dims, input_length=input_length, mask_zero=True))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Bidirectional(layers.GRU(512, return_sequences=True)))\n",
        "  model.add(layers.GRU(512, return_sequences=True))\n",
        "  model.add(layers.Dense(512, activation=tf.nn.relu))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(1024, activation=tf.nn.relu))\n",
        "  model.add(layers.Dropout(0.5))\n",
        "  model.add(layers.Dense(output_len, activation=tf.nn.softmax))\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=OPTIMIZER,\n",
        "      loss=LOSS,\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EMWZXYLLwiwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/training_checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "CHECKPOINT_DIR = os.path.dirname(CHECKPOINT_PATH)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=CHECKPOINT_PATH, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    verbose=1, \n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    eng_total_words, \n",
        "    ind_total_words, \n",
        "    EMBED_DIMS,\n",
        "    maxlen,\n",
        ")\n",
        "\n",
        "model.save_weights(CHECKPOINT_PATH.format(epoch=0))\n",
        "\n",
        "history=model.fit(eng_pad_seqs, \n",
        "                  ind_pad_seqs,\n",
        "                  epochs=EPOCHS,\n",
        "                  batch_size=BATCH_SIZE,\n",
        "                  callbacks=[cp_callback],\n",
        "                  verbose=1)"
      ],
      "metadata": {
        "id": "2cqjvZ5Owjsm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}