{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator amin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoPI-CW88EcR",
        "outputId": "88a04b2f-42b6-4a39-9fb5-6d91b728bca3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.10,>=2.9.0\n",
            "  Downloading tensorflow-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |██████████████████████████████  | 479.8 MB 13.6 MB/s eta 0:00:03\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "56aTseK4q9ol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "746c01cf-e13a-435f-fd4b-0c48e20180b7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0c1aaa7303a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git_dir = \"/content/IOH-Chat-App\"\n",
        "git_url = \"https://github.com/Bangkit-Capstone-Team/IOH-Chat-App.git\"\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call([\"git\", \"clone\", git_url])"
      ],
      "metadata": {
        "id": "2ZtMplOQv6Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filedir):\n",
        "  df = pd.read_csv(filedir)\n",
        "\n",
        "  return df.English.tolist(), df.Indonesia.values.tolist()"
      ],
      "metadata": {
        "id": "tRYdCkfh6Bhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/IOH-Chat-App/Machine Learning/datasets/translate sentence/result/eng-ind.csv\"\n",
        "\n",
        "eng_sentences, ind_sentences = load_data(dataset_dir)\n",
        "\n",
        "print(f\"Num of english sentence: {len(eng_sentences)}\")\n",
        "print(f\"Num of indonesia sentence: {len(ind_sentences)}\")\n",
        "print()\n",
        "print(f\"English example: {eng_sentences[-1]}\")\n",
        "print(f\"Indonesia example: {ind_sentences[-1]}\")"
      ],
      "metadata": {
        "id": "QW7fD1GdrExy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_preprocess(text):\n",
        "  text = tf_text.normalize_utf8(text).numpy().decode()\n",
        "  text = text.lower().strip()\n",
        "  text = text.replace(\"\\t\\n\", \"\")\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "_CM74wxj8uxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentences = np.array(list(map(normalize_and_preprocess, eng_sentences)))\n",
        "ind_sentences = np.array(list(map(normalize_and_preprocess, ind_sentences)))"
      ],
      "metadata": {
        "id": "ERT9afZuHydF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(sentence, max_vocab):\n",
        "  tokenizer = Tokenizer(num_words=max_vocab)\n",
        "  tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "dIHbMKGvrHnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seqs(tokenizer, maxlen=None):\n",
        "  return pad_sequences(tokenizer, maxlen=maxlen, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "ThNTAXptY_0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab = 8000\n",
        "\n",
        "eng_tokenizer = tokenizer(eng_sentences, max_vocab)\n",
        "ind_tokenizer = tokenizer(ind_sentences, max_vocab)\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "ind_tokenizer.fit_on_texts(ind_sentences)\n",
        "\n",
        "eng_encode_example = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "ind_encode_example = ind_tokenizer.texts_to_sequences(ind_sentences)\n",
        "\n",
        "eng_vocab = eng_tokenizer.index_word\n",
        "ind_vocab = ind_tokenizer.index_word\n",
        "\n",
        "eng_decode_example = eng_tokenizer.sequences_to_texts(eng_encode_example)\n",
        "ind_decode_example = ind_tokenizer.sequences_to_texts(ind_encode_example)\n",
        "\n",
        "eng_maxlen = max([len(i)for i in eng_decode_example])\n",
        "ind_maxlen = max([len(i)for i in ind_decode_example])\n",
        "\n",
        "print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "print(f\"English sequences: {eng_encode_example[-1]}\")\n",
        "print()\n",
        "print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "print(f\"Indonesia sequences: {ind_encode_example[-1]}\")"
      ],
      "metadata": {
        "id": "aLMndXAX1zQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "buffer_size = len(eng_sentences)\n",
        "\n",
        "eng_pad_seqs = tf.convert_to_tensor(pad_seqs(eng_encode_example, maxlen=eng_maxlen))\n",
        "ind_pad_seqs = tf.convert_to_tensor(pad_seqs(ind_encode_example, maxlen=eng_maxlen))\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((eng_pad_seqs, ind_pad_seqs)).shuffle(buffer_size).batch(batch_size)\n",
        "\n",
        "# eng_pad_seqs = tf.reshape(eng_pad_seqs, (*eng_pad_seqs.shape, 1))\n",
        "# ind_pad_seqs = tf.reshape(ind_pad_seqs, (*eng_pad_seqs.shape[:-1], 1))\n",
        "\n",
        "for input, target in dataset.take(1):\n",
        "  print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "  print(f\"English sequences: {input}\")\n",
        "  print()\n",
        "  print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "  print(f\"Indonesia sequences: {target}\")"
      ],
      "metadata": {
        "id": "3P9pIKvf-0U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dims = 256\n",
        "epochs = 5\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "       from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_len, output_len, embed_dims, maxlen):\n",
        "  # # Model Architecure 1\n",
        "  # en_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  # en_embedding = layers.Embedding(input_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  # en_lstm = layers.LSTM(512, return_state=True)\n",
        "  # en_outputs, state_h, state_c = en_lstm(en_inputs)\n",
        "  # en_states = [state_h, state_c]\n",
        "\n",
        "  # dec_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  # dec_embedding = layers.Embedding(output_len, embed_dims)(dec_inputs)\n",
        "  \n",
        "  # dec_lstm = layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "  # dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)\n",
        "  # x = layers.Dense(1024, activation=tf.nn.relu)(dec_outputs)\n",
        "  # x = layers.Dropout(.5)(x)\n",
        "  # outputs = layers.Dense(output_len, activation=tf.nn.softmax)(x)\n",
        "\n",
        "  # model = Model([en_inputs, dec_inputs], outputs)\n",
        "\n",
        "  # Model Architecure 2\n",
        "  en_inputs = layers.Input(shape=(maxlen,))\n",
        "  en_embedding = layers.Embedding(input_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  en_gru = layers.GRU(512, return_sequences=True, return_state=True,)\n",
        "  en_outputs, en_state = en_gru(en_embedding)\n",
        "\n",
        "  dec_embedding = layers.Embedding(output_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  dec_gru = layers.GRU(512, return_sequences=True, return_state=True)\n",
        "  dec_outputs, dec_state = dec_gru(dec_embedding, initial_state=en_state)\n",
        "\n",
        "  x = layers.Dense(512, activation=tf.nn.relu)(dec_outputs)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  x = layers.Dense(1024, activation=tf.nn.relu)(dec_outputs)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  outputs = layers.Dense(output_len, activation=tf.nn.softmax)(x)\n",
        "\n",
        "  model = Model(en_inputs, outputs)\n",
        "\n",
        "  # # Model Architecure 3\n",
        "  # model = tf.keras.Sequential()\n",
        "  \n",
        "  # model.add(layers.Input(shape=(maxlen,)))\n",
        "  # model.add(layers.Embedding(input_len, embed_dims))\n",
        "  # model.add(layers.GRU(512, return_sequences=True, return_state=True))\n",
        "  # model.add(layers.Embedding(output_len, embed_dims))\n",
        "  # model.add(layers.GRU(512, return_sequences=True, return_state=True))  \n",
        "  # model.add(layers.Dense(512, activation=tf.nn.relu))\n",
        "  # model.add(layers.Dropout(.5))\n",
        "  # model.add(layers.Dense(1024, activation=tf.nn.relu))\n",
        "  # model.add(layers.Dropout(.5))\n",
        "  # model.add(layers.Dense(output_len, activation=tf.nn.softmax))\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EMWZXYLLwiwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/training_checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq=10,\n",
        "    verbose=1, \n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    len(eng_vocab) + 1, \n",
        "    len(ind_vocab) + 1, \n",
        "    embed_dims,\n",
        "    eng_maxlen,\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.save_weights(checkpoint_path.format(epoch=0))"
      ],
      "metadata": {
        "id": "2cqjvZ5Owjsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset,\n",
        "          epochs=epochs,\n",
        "          callbacks=[cp_callback],\n",
        "          verbose=1)"
      ],
      "metadata": {
        "id": "HJygvxEGlUWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/saved_model/model.h5\"\n",
        "saved_model_dir = os.path.dirname(saved_model_path)\n",
        "\n",
        "if os.path.exists(saved_model_dir):\n",
        "  shutil.rmtree(saved_model_dir)\n",
        "\n",
        "model.save(saved_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "0QGxWYuiQPnv",
        "outputId": "a71bf637-eb86-4fa9-a04f-f0d1e054e7e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8ee952b02fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msaved_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/IOH-Chat-App/Machine Learning/code/translate sentence/saved_model/model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaved_model_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "fXrqo0owQrVo",
        "outputId": "f3b8f001-60d8-4bd2-9322-6c1091c14cfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-64e6a3f1b807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlatest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlatest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "  en_outputs, en_state = model.layers[2].output\n",
        "  en_model = Model(model.input[0], en_state)\n",
        "\n",
        "  dec_state = layers.Input(shape=(512,))\n",
        "  dec_inputs = dec_state\n",
        "\n",
        "  dec_gru = model.layers[3]\n",
        "  dec_outputs, dec_state= dec_gru(model.input[0], initial_state=dec_inputs)\n",
        "  \n",
        "  dec_dense1 = model.layers[4](dec_outputs)\n",
        "  dec_dropout1 = model.layers[5](dec_dense1)\n",
        "  dec_dense2 = model.layers[6](dec_dropout1)\n",
        "  dec_dropout2 = model.layers[7](dec_dense2)\n",
        "\n",
        "  dec_model = Model(model.input[0] + dec_inputs, \n",
        "                    [dec_dropout2] + dec_state)\n",
        "  \n",
        "  return en_model, dec_model"
      ],
      "metadata": {
        "id": "kcJQFJJrhTzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_model, dec_model = load_model(saved_model_path)"
      ],
      "metadata": {
        "id": "rh4i5L5hEgKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "  tokens = list()\n",
        "\n",
        "  sequences = eng_tokenizer.texts_to_sequences([text])\n",
        "  sequences = tf.convert_to_tensor(pad_seqs(sequences))\n",
        "\n",
        "  input = en_model.predict(sequences)\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  \n",
        "  for i in sequences:\n",
        "    output_chars, h, c = dec_model.predict([target_seq] + input)\n",
        "    char_index = np.argmax(output_chars)\n",
        "    text_char = ind_tokenizer.index_word[char_index]\n",
        "    tokens.append(text_char)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = char_index\n",
        "    states_value = [h, c]\n",
        "  \n",
        "  sentence = \" \".join(tokens)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "vY1J-UH1kfmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"if a person has not had a chance to acquire his target language by the time\")"
      ],
      "metadata": {
        "id": "PcvPTORhE5ph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}