{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator amin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoPI-CW88EcR",
        "outputId": "e0ea22ff-cac5-4295-9b71-4dcc13d0c9c1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.25.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.46.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (14.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "56aTseK4q9ol"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git_dir = \"/content/IOH-Chat-App\"\n",
        "git_url = \"https://github.com/Bangkit-Capstone-Team/IOH-Chat-App.git\"\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call([\"git\", \"clone\", git_url])"
      ],
      "metadata": {
        "id": "2ZtMplOQv6Lz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filedir):\n",
        "  df = pd.read_csv(filedir)\n",
        "\n",
        "  return df.English.tolist(), df.Indonesia.values.tolist()"
      ],
      "metadata": {
        "id": "tRYdCkfh6Bhw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/IOH-Chat-App/Machine Learning/datasets/translate sentence/result/eng-ind.csv\"\n",
        "\n",
        "eng_sentences, ind_sentences = load_data(dataset_dir)\n",
        "\n",
        "print(f\"Num of english sentence: {len(eng_sentences)}\")\n",
        "print(f\"Num of indonesia sentence: {len(ind_sentences)}\")\n",
        "print()\n",
        "print(f\"English example: {eng_sentences[-1]}\")\n",
        "print(f\"Indonesia example: {ind_sentences[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW7fD1GdrExy",
        "outputId": "a824a068-1b5d-4a9f-fbb1-2fcb40716885"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of english sentence: 8819\n",
            "Num of indonesia sentence: 8819\n",
            "\n",
            "English example: If a person has not had a chance to acquire his target language by the time he's an adult, he's unlikely to be able to reach native speaker level in that language.\n",
            "Indonesia example: Jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa, maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_preprocess(text):\n",
        "  text = tf_text.normalize_utf8(text).numpy().decode()\n",
        "  text = text.lower().strip()\n",
        "  text = text.replace(\"\\t\\n\", \"\")\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "_CM74wxj8uxy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentences = np.array(list(map(normalize_and_preprocess, eng_sentences)))\n",
        "ind_sentences = np.array(list(map(normalize_and_preprocess, ind_sentences)))"
      ],
      "metadata": {
        "id": "ERT9afZuHydF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(sentence, max_vocab):\n",
        "  tokenizer = Tokenizer(num_words=max_vocab)\n",
        "  tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "dIHbMKGvrHnD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_seqs(tokenizer, maxlen=None):\n",
        "  return pad_sequences(tokenizer, maxlen=maxlen, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "ThNTAXptY_0A"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab = 8000\n",
        "\n",
        "eng_tokenizer = tokenizer(eng_sentences, max_vocab)\n",
        "ind_tokenizer = tokenizer(ind_sentences, max_vocab)\n",
        "\n",
        "eng_tokenizer.fit_on_texts(eng_sentences)\n",
        "ind_tokenizer.fit_on_texts(ind_sentences)\n",
        "\n",
        "eng_encode_example = eng_tokenizer.texts_to_sequences(eng_sentences)\n",
        "ind_encode_example = ind_tokenizer.texts_to_sequences(ind_sentences)\n",
        "\n",
        "eng_vocab = eng_tokenizer.index_word\n",
        "ind_vocab = ind_tokenizer.index_word\n",
        "\n",
        "eng_decode_example = eng_tokenizer.sequences_to_texts(eng_encode_example)\n",
        "ind_decode_example = ind_tokenizer.sequences_to_texts(ind_encode_example)\n",
        "\n",
        "eng_maxlen = max([len(i)for i in eng_decode_example])\n",
        "ind_maxlen = max([len(i)for i in ind_decode_example])\n",
        "\n",
        "\n",
        "print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "print(f\"English sequences: {eng_encode_example[-1]}\")\n",
        "print()\n",
        "print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "print(f\"Indonesia sequences: {ind_encode_example[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLMndXAX1zQD",
        "outputId": "82e56533-d05e-4b83-bce2-4f599ae9ab17"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence: if a person has not had a chance to acquire his target language by the time he's an adult he's unlikely to be able to reach native speaker level in that language\n",
            "English sequences: [70, 7, 448, 42, 28, 63, 7, 692, 5, 4087, 32, 4088, 660, 78, 3, 43, 127, 77, 4089, 127, 2249, 5, 25, 258, 5, 1290, 1641, 1623, 4090, 15, 11, 660]\n",
            "\n",
            "Indonesia sentence: jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut\n",
            "Indonesia sequences: [119, 290, 3, 4869, 17, 1607, 71, 4, 4870, 158, 4871, 1165, 823, 242, 534, 80, 12, 15, 1719, 4872, 4873, 1160, 46, 71, 248]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pad_seqs = tf.convert_to_tensor(pad_seqs(eng_encode_example, maxlen=eng_maxlen))\n",
        "ind_pad_seqs = tf.convert_to_tensor(pad_seqs(ind_encode_example, maxlen=eng_maxlen))\n",
        "\n",
        "# eng_pad_seqs = tf.reshape(eng_pad_seqs, (*eng_pad_seqs.shape, 1))\n",
        "# ind_pad_seqs = tf.reshape(ind_pad_seqs, (*eng_pad_seqs.shape[:-1], 1))\n",
        "\n",
        "print(f\"English sentence: {eng_decode_example[-1]}\")\n",
        "print(f\"English sequences: {eng_pad_seqs[-1]}\")\n",
        "print()\n",
        "print(f\"Indonesia sentence: {ind_decode_example[-1]}\")\n",
        "print(f\"Indonesia sequences: {ind_pad_seqs[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P9pIKvf-0U0",
        "outputId": "320748f5-9588-46a1-c27b-e75c367b82bb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English sentence: if a person has not had a chance to acquire his target language by the time he's an adult he's unlikely to be able to reach native speaker level in that language\n",
            "English sequences: [  70    7  448   42   28   63    7  692    5 4087   32 4088  660   78\n",
            "    3   43  127   77 4089  127 2249    5   25  258    5 1290 1641 1623\n",
            " 4090   15   11  660    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n",
            "\n",
            "Indonesia sentence: jika seseorang tidak berkesempatan untuk menguasai bahasa yang diinginkannya ketika menginjak dewasa maka kecil kemungkinan ia akan bisa mencapai tingkatan penutur asli dalam bahasa tersebut\n",
            "Indonesia sequences: [ 119  290    3 4869   17 1607   71    4 4870  158 4871 1165  823  242\n",
            "  534   80   12   15 1719 4872 4873 1160   46   71  248    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dims = 256\n",
        "lr = 1e-4\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "loss = tf.keras.losses.sparse_categorical_crossentropy"
      ],
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_len, output_len, embed_dims, maxlen):\n",
        "  # # Model Architecure 1\n",
        "\n",
        "  # en_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  # en_embedding = layers.Embedding(input_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  # en_lstm = layers.LSTM(512, return_state=True)\n",
        "  # en_outputs, state_h, state_c = en_lstm(en_inputs)\n",
        "  # en_states = [state_h, state_c]\n",
        "\n",
        "  # dec_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  # dec_embedding = layers.Embedding(output_len, embed_dims)(dec_inputs)\n",
        "  \n",
        "  # dec_lstm = layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "  # dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)\n",
        "  # x = layers.Dense(1024, activation=tf.nn.relu)(dec_outputs)\n",
        "  # x = layers.Dropout(.5)(x)\n",
        "  # outputs = layers.Dense(output_len, activation=tf.nn.softmax)(x)\n",
        "\n",
        "  # model = Model([en_inputs, dec_inputs], outputs)\n",
        "\n",
        "  # Model Architecure 2\n",
        "\n",
        "  en_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  en_embedding = layers.Embedding(input_len, embed_dims)(en_inputs)\n",
        "  \n",
        "  en_lstm = layers.LSTM(512, return_state=True)\n",
        "  en_outputs, state_h, state_c = en_lstm(en_inputs)\n",
        "  en_states = [state_h, state_c]\n",
        "\n",
        "  dec_inputs = layers.Input(shape=(maxlen, 1))\n",
        "  dec_embedding = layers.Embedding(output_len, embed_dims)(dec_inputs)\n",
        "  \n",
        "  dec_lstm = layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "  dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)\n",
        "  x = layers.Dense(512, activation=tf.nn.relu)(dec_outputs)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  x = layers.Dense(1024, activation=tf.nn.relu)(dec_outputs)\n",
        "  x = layers.Dropout(.5)(x)\n",
        "  outputs = layers.Dense(output_len, activation=tf.nn.softmax)(x)\n",
        "\n",
        "  model = Model([en_inputs, dec_inputs], outputs)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EMWZXYLLwiwP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/training_checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq=5,\n",
        "    verbose=1, \n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    len(eng_vocab) + 1, \n",
        "    len(ind_vocab) + 1, \n",
        "    embed_dims,\n",
        "    eng_maxlen,\n",
        ")\n",
        "\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "history=model.fit([eng_pad_seqs, ind_pad_seqs], \n",
        "                  ind_pad_seqs,\n",
        "                  epochs=epochs,\n",
        "                  batch_size=batch_size,\n",
        "                  callbacks=[cp_callback],\n",
        "                  validation_split=0.2,\n",
        "                  verbose=1)"
      ],
      "metadata": {
        "id": "2cqjvZ5Owjsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c4b052-364b-4034-a710-58b49bce8b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)           [(None, 161, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 161, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  [(None, 512),        1052672     ['input_8[0][0]']                \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)                  [(None, 161, 512),   1052672     ['input_9[0][0]',                \n",
            "                                 (None, 512),                     'lstm_4[0][1]',                 \n",
            "                                 (None, 512)]                     'lstm_4[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 161, 1024)    525312      ['lstm_5[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 161, 1024)    0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 161, 4874)    4995850     ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,626,506\n",
            "Trainable params: 7,626,506\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/15\n",
            "  4/111 [>.............................] - ETA: 25:36 - loss: 8.4776 - accuracy: 0.7464WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "  9/111 [=>............................] - ETA: 23:09 - loss: 8.4527 - accuracy: 0.8714WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 14/111 [==>...........................] - ETA: 22:05 - loss: 8.4167 - accuracy: 0.9070WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 19/111 [====>.........................] - ETA: 20:43 - loss: 8.3361 - accuracy: 0.9241WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 24/111 [=====>........................] - ETA: 19:24 - loss: 7.8638 - accuracy: 0.9341WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 29/111 [======>.......................] - ETA: 18:07 - loss: 6.9155 - accuracy: 0.9406WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 34/111 [========>.....................] - ETA: 16:59 - loss: 5.9769 - accuracy: 0.9452WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 39/111 [=========>....................] - ETA: 15:52 - loss: 5.2439 - accuracy: 0.9486WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 44/111 [==========>...................] - ETA: 14:47 - loss: 4.6752 - accuracy: 0.9511WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 49/111 [============>.................] - ETA: 13:41 - loss: 4.2217 - accuracy: 0.9533WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 54/111 [=============>................] - ETA: 12:33 - loss: 3.8518 - accuracy: 0.9550WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 59/111 [==============>...............] - ETA: 11:23 - loss: 3.5449 - accuracy: 0.9563WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 64/111 [================>.............] - ETA: 10:32 - loss: 3.2852 - accuracy: 0.9574WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 69/111 [=================>............] - ETA: 9:27 - loss: 3.0622 - accuracy: 0.9584WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 74/111 [===================>..........] - ETA: 8:15 - loss: 2.8688 - accuracy: 0.9594WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 79/111 [====================>.........] - ETA: 7:17 - loss: 2.6992 - accuracy: 0.9602WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 84/111 [=====================>........] - ETA: 6:09 - loss: 2.5499 - accuracy: 0.9609WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 89/111 [=======================>......] - ETA: 5:05 - loss: 2.4169 - accuracy: 0.9616WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 94/111 [========================>.....] - ETA: 3:57 - loss: 2.2982 - accuracy: 0.9622WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 99/111 [=========================>....] - ETA: 2:48 - loss: 2.1915 - accuracy: 0.9627WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "104/111 [===========================>..] - ETA: 1:40 - loss: 2.0952 - accuracy: 0.9631WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "109/111 [============================>.] - ETA: 28s - loss: 2.0074 - accuracy: 0.9635WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "111/111 [==============================] - 1739s 16s/step - loss: 1.9869 - accuracy: 0.9636 - val_loss: 0.3409 - val_accuracy: 0.9498\n",
            "Epoch 2/15\n",
            "  3/111 [..............................] - ETA: 21:09 - loss: 0.1869 - accuracy: 0.9712WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "  8/111 [=>............................] - ETA: 20:17 - loss: 0.1785 - accuracy: 0.9725WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 13/111 [==>...........................] - ETA: 19:11 - loss: 0.1783 - accuracy: 0.9725WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            " 15/111 [===>..........................] - ETA: 19:21 - loss: 0.1791 - accuracy: 0.9724"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_dir = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/saved_model/model.h5\"\n",
        "\n",
        "model.save(saved_model_dir)"
      ],
      "metadata": {
        "id": "0QGxWYuiQPnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "metadata": {
        "id": "fXrqo0owQrVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "  en_outputs, state_h_en, state_c_en = model.layers[2].output\n",
        "  en_model = Model(model.input[0], [state_h_en, state_c_en])\n",
        "\n",
        "  dec_state_h = layers.Input(shape=(128,))\n",
        "  dec_state_c = layers.Input(shape=(128,))\n",
        "  dec_inputs = [dec_state_h, dec_state_c]\n",
        "\n",
        "  dec_lstm = model.layers[3]\n",
        "  dec_outputs, state_h_dec, state_c_dec = dec_lstm(model.input[1], \n",
        "                                                   initial_state=dec_inputs)\n",
        "  \n",
        "  dec_states = [state_h_dec, state_c_dec]\n",
        "  dec_dense = model.layers[6]\n",
        "  dec_dense = dec_dense(dec_outputs)\n",
        "  dec_model = Model([model.input[1]] + dec_inputs, \n",
        "                    [dec_dense] + dec_states)\n",
        "  \n",
        "  return en_model, dec_model"
      ],
      "metadata": {
        "id": "kcJQFJJrhTzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_to_text(en_model, dec_model, sequences):\n",
        "  input = en_model.predict(sequences)"
      ],
      "metadata": {
        "id": "vY1J-UH1kfmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}