{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator amin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoPI-CW88EcR",
        "outputId": "2f344957-f2ad-475a-9fe2-976e34097a18"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (14.0.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (0.25.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow_text) (1.46.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.3.7)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow_text) (3.0.9)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "56aTseK4q9ol"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import tensorflow_addons as tfa\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tz4aY7oDbyeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git_dir = \"/content/IOH-Chat-App\"\n",
        "git_url = \"https://github.com/Bangkit-Capstone-Team/IOH-Chat-App.git\"\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call([\"git\", \"clone\", git_url])"
      ],
      "metadata": {
        "id": "2ZtMplOQv6Lz"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatorDataset:\n",
        "  \n",
        "  def __init__(self, filedir):\n",
        "    self.filedir = filedir\n",
        "    self.input_tokenizer = None\n",
        "    self.target_tokenizer = None\n",
        "    self._load_data_from_file()\n",
        "\n",
        "  def _load_data_from_file(self):\n",
        "    df = pd.read_csv(self.filedir)\n",
        "\n",
        "    self.input_lang = df.English.tolist()\n",
        "    self.target_lang = df.Indonesia.values.tolist()\n",
        "\n",
        "  def normalize_and_preprocess(self, text):\n",
        "    text = tf_text.normalize_utf8(text).numpy().decode()\n",
        "    text = text.lower().strip()\n",
        "    text = text.replace(\"\\t\\n\", \"\")\n",
        "\n",
        "    return text\n",
        "\n",
        "  def _create_dataset(self):\n",
        "    self.input_lang = np.array(list(map(self.normalize_and_preprocess, self.input_lang)))\n",
        "    self.target_lang = np.array(list(map(self.normalize_and_preprocess, self.target_lang)))\n",
        "    \n",
        "    return self.input_lang, self.target_lang\n",
        "\n",
        "  def _tokenize(self, sentence, num_words, maxlen):\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "    tokenizer.fit_on_texts(sentence)\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(sentence)\n",
        "    sequences = pad_sequences(sequences, maxlen, padding=\"post\")\n",
        "\n",
        "    return sequences, tokenizer\n",
        "\n",
        "  def _load_dataset(self, num_words):\n",
        "    input_lang, target_lang = self._create_dataset()\n",
        "\n",
        "    maxlen = max([len(i)for i in target_lang])\n",
        "\n",
        "    input_sequences, input_tokenizer = self._tokenize(input_lang, num_words, maxlen)\n",
        "    target_sequences, target_tokenizer = self._tokenize(target_lang, num_words, maxlen)\n",
        "\n",
        "    return (input_sequences, input_tokenizer), (target_sequences, target_tokenizer)\n",
        "  \n",
        "  def call(self, num_words, batch_size, buffer_size):\n",
        "    input, target = self._load_dataset(num_words)\n",
        "\n",
        "    input_sequences, self.input_tokenizer = input\n",
        "    target_sequences, self.target_tokenizer = target\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return self.input_tokenizer, self.target_tokenizer, dataset"
      ],
      "metadata": {
        "id": "KA5VqxGKtrWx"
      },
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size = 8000\n",
        "batch_size = 128\n",
        "num_words = 500\n",
        "\n",
        "dataset_dir = \"/content/IOH-Chat-App/Machine Learning/datasets/translate sentence/result/eng-ind.csv\"\n",
        "\n",
        "translator_dataset = TranslatorDataset(dataset_dir)\n",
        "input_tokenizer, target_tokenizer, dataset = translator_dataset.call(num_words, \n",
        "                                                                     batch_size, \n",
        "                                                                     buffer_size)\n",
        "\n",
        "input_batch, target_batch = next(iter(dataset))\n",
        "input_batch.shape, target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW7fD1GdrExy",
        "outputId": "ab6482af-b8fe-47bd-fd44-5047d6703a92"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 192]), TensorShape([128, 192]))"
            ]
          },
          "metadata": {},
          "execution_count": 393
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "input_maxlen = input_batch.shape[1]\n",
        "target_maxlen = target_batch.shape[1]\n",
        "\n",
        "input_maxlen, target_maxlen, input_vocab_size, target_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI-TplUE22Tk",
        "outputId": "77e4e158-122b-4afb-e873-f0d1dae969e0"
      },
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(192, 192, 4091, 4874)"
            ]
          },
          "metadata": {},
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq():\n",
        "\n",
        "  def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, units, batch_size, maxlen):\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.maxlen = maxlen\n",
        "    self.batch_size = batch_size\n",
        "    self.units = units\n",
        "    self.en_embedding = layers.Embedding(self.input_vocab_size, embedding_dim)\n",
        "    self.dec_embedding = layers.Embedding(self.input_vocab_size, embedding_dim)\n",
        "    self.en_lstm_layer = layers.LSTM(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    self.dec_lstm_layer = layers.LSTM(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True)\n",
        "\n",
        "  def _create_dense(self, input):\n",
        "    x = layers.Dense(512, activation=tf.nn.relu)(input)\n",
        "    x = layers.Dropout(.5)(x)\n",
        "    x = layers.Dense(1024, activation=tf.nn.relu)(x)\n",
        "    x = layers.Dropout(.5)(x)\n",
        "    outputs = layers.TimeDistributed(layers.Dense(self.output_vocab_size, activation=tf.nn.softmax))(x)\n",
        "    return outputs\n",
        "\n",
        "  def encoder(self, input):\n",
        "    embedding = self.en_embedding(input)\n",
        "    output, h, c = self.en_lstm_layer(embedding)\n",
        "\n",
        "    return output, h, c\n",
        "\n",
        "  def decoder(self, input, encoder_state):\n",
        "    embedding = self.dec_embedding(input)\n",
        "    outputs, _, _ = self.dec_lstm_layer(embedding, \n",
        "                                        initial_state=encoder_state)\n",
        "    outputs = self._create_dense(outputs)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "VixmOkyJ7s_B"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dims = 256\n",
        "epochs = 5\n",
        "units = 512\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "       from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2Seq(input_vocab_size, \n",
        "                  target_vocab_size, \n",
        "                  embed_dims, \n",
        "                  units, \n",
        "                  batch_size, \n",
        "                  target_maxlen)"
      ],
      "metadata": {
        "id": "24cbBI6f6J2R"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_outputs, en_h_state, en_c_state = seq2seq.encoder(input_batch)\n",
        "\n",
        "print(en_outputs.shape)\n",
        "print(en_h_state.shape)\n",
        "print(en_c_state.shape)"
      ],
      "metadata": {
        "id": "ciAQcC-GBjal",
        "outputId": "148a132c-6b23-435f-cc3b-c908e5f1c14f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 409,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 192, 512)\n",
            "(128, 512)\n",
            "(128, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dec_outputs = seq2seq.decoder(target_batch, [en_h_state, en_c_state])\n",
        "\n",
        "print(dec_outputs.shape)"
      ],
      "metadata": {
        "id": "zo43VZr2C_ZQ",
        "outputId": "92b7ec4c-09f3-4eb7-cd3b-4cf51531902e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 192, 4874)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(seq2seq, batch_size, shape):\n",
        "  en_inputs = layers.Input(shape=(shape[1],))\n",
        "  en_outputs, en_h_state, en_c_state = seq2seq.encoder(en_inputs)\n",
        "  dec_outputs = seq2seq.decoder(en_inputs, [en_h_state, en_c_state])\n",
        "\n",
        "  model = Model(en_inputs, dec_outputs)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "EMWZXYLLwiwP"
      },
      "execution_count": 411,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/training_checkpoints/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq=10,\n",
        "    verbose=1, \n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    seq2seq, \n",
        "    batch_size, \n",
        "    input_batch.shape\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.save_weights(checkpoint_path.format(epoch=0))"
      ],
      "metadata": {
        "id": "2cqjvZ5Owjsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9630daea-9111-453c-a41a-6a13811e978d"
      },
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_20\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_46 (InputLayer)          [(None, 192)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_87 (Embedding)       (None, 192, 256)     1047296     ['input_46[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_88 (Embedding)       (None, 192, 256)     1047296     ['input_46[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_59 (LSTM)                 [(None, 192, 512),   1574912     ['embedding_87[0][0]']           \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " lstm_60 (LSTM)                 [(None, 192, 512),   1574912     ['embedding_88[0][0]',           \n",
            "                                 (None, 512),                     'lstm_59[0][1]',                \n",
            "                                 (None, 512)]                     'lstm_59[0][2]']                \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 192, 512)     262656      ['lstm_60[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 192, 512)     0           ['dense_98[0][0]']               \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 192, 1024)    525312      ['dropout_52[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_53 (Dropout)           (None, 192, 1024)    0           ['dense_99[0][0]']               \n",
            "                                                                                                  \n",
            " time_distributed_17 (TimeDistr  (None, 192, 4874)   4995850     ['dropout_53[0][0]']             \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,028,234\n",
            "Trainable params: 11,028,234\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset,\n",
        "          epochs=epochs,\n",
        "          callbacks=[cp_callback],\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJygvxEGlUWz",
        "outputId": "ba7f33e4-badf-4cfa-ad88-adbb65126a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 9/68 [==>...........................] - ETA: 35:26 - loss: 7.9638 - accuracy: 0.8709WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "18/68 [======>.......................] - ETA: 30:18 - loss: 7.7390 - accuracy: 0.9244"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"/content/IOH-Chat-App/Machine Learning/code/translate sentence/saved_model/model.h5\"\n",
        "saved_model_dir = os.path.dirname(saved_model_path)\n",
        "\n",
        "if os.path.exists(saved_model_dir):\n",
        "  shutil.rmtree(saved_model_dir)\n",
        "else:\n",
        "  model.save(saved_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "0QGxWYuiQPnv",
        "outputId": "a71bf637-eb86-4fa9-a04f-f0d1e054e7e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8ee952b02fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msaved_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/IOH-Chat-App/Machine Learning/code/translate sentence/saved_model/model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaved_model_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "fXrqo0owQrVo",
        "outputId": "f3b8f001-60d8-4bd2-9322-6c1091c14cfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-64e6a3f1b807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlatest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlatest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path):\n",
        "  model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "  en_outputs, en_state = model.layers[3].output\n",
        "  en_model = Model(model.input[0], en_state)\n",
        "\n",
        "  dec_state = layers.Input(shape=(512,))\n",
        "  dec_inputs = dec_state\n",
        "\n",
        "  dec_gru = model.layers[4]\n",
        "  dec_outputs, dec_state= dec_gru(model.input[0], initial_state=dec_inputs)\n",
        "  \n",
        "  dec_dense1 = model.layers[5](dec_outputs)\n",
        "  dec_dropout1 = model.layers[6](dec_dense1)\n",
        "  dec_dense2 = model.layers[7](dec_dropout1)\n",
        "  dec_dropout2 = model.layers[8](dec_dense2)\n",
        "  output = model.layers[9](dec_dense2)\n",
        "\n",
        "  dec_model = Model(model.input[0] + dec_inputs, \n",
        "                    [output] + dec_state)\n",
        "  \n",
        "  return en_model, dec_model"
      ],
      "metadata": {
        "id": "kcJQFJJrhTzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_model, dec_model = load_model(saved_model_path)"
      ],
      "metadata": {
        "id": "rh4i5L5hEgKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "  tokens = list()\n",
        "\n",
        "  sequences = eng_tokenizer.texts_to_sequences([text])\n",
        "  sequences = tf.convert_to_tensor(pad_seqs(sequences))\n",
        "\n",
        "  input = en_model.predict(sequences)\n",
        "  target_seq = np.zeros((1, 1))\n",
        "  \n",
        "  for i in sequences:\n",
        "    output_chars, h, c = dec_model.predict([target_seq] + input)\n",
        "    char_index = np.argmax(output_chars)\n",
        "    text_char = ind_tokenizer.index_word[char_index]\n",
        "    tokens.append(text_char)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = char_index\n",
        "    states_value = [h, c]\n",
        "  \n",
        "  sentence = \" \".join(tokens)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "vY1J-UH1kfmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"if a person has not had a chance to acquire his target language by the time\")"
      ],
      "metadata": {
        "id": "PcvPTORhE5ph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}