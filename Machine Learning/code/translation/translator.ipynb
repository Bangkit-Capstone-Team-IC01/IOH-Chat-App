{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "56aTseK4q9ol"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import typing\n",
        "\n",
        "from typing import Any, Tuple\n",
        "from google.colab import drive #if use colab\n",
        "from tensorflow.nn import relu, tanh, softmax\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tz4aY7oDbyeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb9068d-4a14-40db-b963-6d185f5cf706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# if use colab\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2ZtMplOQv6Lz"
      },
      "outputs": [],
      "source": [
        "#if use colab\n",
        "git_dir = \"/content/IOH-Chat-App\"\n",
        "git_url = \"https://github.com/Bangkit-Capstone-Team/IOH-Chat-App.git\"\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call([\"git\", \"clone\", git_url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pN2M4PirLEQB"
      },
      "outputs": [],
      "source": [
        "filedir = \"/content/IOH-Chat-App/Machine Learning/datasets/translate sentence/result/eng-ind.csv\" # #if use colab\n",
        "# filedir = \"../..//datasets/translate sentence/result/eng-ind.csv\" #if use local env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_mark = \"<START>\"\n",
        "end_mark = \"<END>\""
      ],
      "metadata": {
        "id": "RgUUgmronChh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KA5VqxGKtrWx"
      },
      "outputs": [],
      "source": [
        "class TranslatorDataset:\n",
        "  def __init__(self, filedir):\n",
        "    self.filedir = filedir\n",
        "    self.input_tokenizer = None\n",
        "    self.target_tokenizer = None\n",
        "    self._load_data_from_file()\n",
        "\n",
        "  def _load_data_from_file(self):\n",
        "    df = pd.read_csv(self.filedir)\n",
        "\n",
        "    input_lang = df.English.values\n",
        "    target_lang = df.Indonesia.values\n",
        "\n",
        "    return input_lang, target_lang\n",
        "\n",
        "  def _normalize_and_preprocess(self, text):\n",
        "    punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    \n",
        "    text = text.lower().strip()\n",
        "    text = text.replace(punctuation, \"\")\n",
        "    text = start_mark + \" \" + text\n",
        "    text = text + \" \" + end_mark\n",
        "\n",
        "    return text\n",
        "\n",
        "  def _tokenize(self, sentences, num_words, maxlen, padding, truncating):\n",
        "    tokenizer = Tokenizer(num_words=num_words, filters=\"\", lower=False)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    sequences = pad_sequences(sequences, \n",
        "                              maxlen, \n",
        "                              padding=padding, \n",
        "                              truncating=truncating)\n",
        "\n",
        "    return sequences, tokenizer\n",
        "\n",
        "  def _create_dataset(self):\n",
        "    input_lang, target_lang = self._load_data_from_file()\n",
        "\n",
        "    input_lang = np.array(list(map(self._normalize_and_preprocess, input_lang)))\n",
        "    target_lang = np.array(list(map(self._normalize_and_preprocess, target_lang)))\n",
        "    \n",
        "    return input_lang, target_lang\n",
        "\n",
        "  def _load_dataset(self, num_words):\n",
        "    input_lang, target_lang = self._create_dataset()\n",
        "\n",
        "    self.maxlen = max([len(i)for i in input_lang]) // 5\n",
        "\n",
        "    input_sequences, input_tokenizer = self._tokenize(input_lang, \n",
        "                                                      num_words, \n",
        "                                                      self.maxlen,\n",
        "                                                      \"pre\", \"pre\")\n",
        "    target_sequences, target_tokenizer = self._tokenize(target_lang, \n",
        "                                                        num_words, \n",
        "                                                        self.maxlen,\n",
        "                                                        \"post\", \"post\")\n",
        "\n",
        "    return (input_sequences, input_tokenizer), (target_sequences, target_tokenizer)\n",
        "  \n",
        "  def get(self, num_words):\n",
        "    input, target = self._load_dataset(num_words)\n",
        "\n",
        "    input_sequences, self.input_tokenizer = input\n",
        "    target_sequences, self.target_tokenizer = target\n",
        "\n",
        "    dataset = (input_sequences, target_sequences)\n",
        "\n",
        "    return self.input_tokenizer, self.target_tokenizer, dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 6000"
      ],
      "metadata": {
        "id": "8lYoZZjqXF__"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QW7fD1GdrExy"
      },
      "outputs": [],
      "source": [
        "translator_dataset = TranslatorDataset(filedir)\n",
        "input_tokenizer, target_tokenizer, (input_sequences, target_sequences) = translator_dataset.get(num_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size = len(input_sequences)\n",
        "batch_size = 64\n",
        "steps_per_epoch = buffer_size // batch_size"
      ],
      "metadata": {
        "id": "YErjmG5MW-ZS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "metadata": {
        "id": "EfnlbRAmXTMk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch, target_batch = next(iter(dataset))"
      ],
      "metadata": {
        "id": "3lQitbaJXt4O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch.shape, target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j91LroJX50x",
        "outputId": "1a7b5540-5e90-4f90-8148-1b30bb3c905c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 35]), TensorShape([64, 35]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cI-TplUE22Tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05b1914-de7c-4cca-a5c4-bb41f94916d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35, 35, 6139, 6865)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "input_vocab_size = len(input_tokenizer.index_word) + 1\n",
        "target_vocab_size = len(target_tokenizer.index_word) + 1\n",
        "input_maxlen = input_sequences.shape[1]\n",
        "target_maxlen = target_sequences.shape[1]\n",
        "\n",
        "input_maxlen, target_maxlen, input_vocab_size, target_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_example = input_sequences[-1]\n",
        "input_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPp4UAxdxemM",
        "outputId": "c60de6c8-a3c2-4ee1-cf2e-2faca9453db7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    1,   66,    9,  859,   42,   28,\n",
              "         61,    9, 1018,    5,   32, 1013,   76,    4,   73,  120,   74,\n",
              "        120, 2949,    5,   26,  225,    5, 1475, 1960, 2983,   14,   21,\n",
              "       1373,    2], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_example = target_sequences[-1]\n",
        "target_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r5qpp8Hxjzk",
        "outputId": "75b88fdd-9d93-4724-d0f9-c4b24f1ee599"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,  103,  291,    5,   19, 1785,   67,    6,  135, 2853,  841,\n",
              "        552,  528,   71,   12,   17, 1936, 1798,   45,   67,  431,    2,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sens_example = input_tokenizer.sequences_to_texts([input_example])\n",
        "input_sens_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2w5hptxxrhq",
        "outputId": "331be624-82e1-46bd-bf05-a3a60e7233f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"<START> if a person has not had a chance to his language by the time he's an he's unlikely to be able to reach native speaker in that language. <END>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_sens_example = target_tokenizer.sequences_to_texts([target_example])\n",
        "target_sens_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDHp7Mm1yClM",
        "outputId": "f21027fb-9a90-4e49-ff42-e9e44abccf14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> jika seseorang tidak untuk menguasai bahasa yang ketika dewasa, maka kecil kemungkinan ia akan bisa mencapai asli dalam bahasa tersebut. <END>']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "outputs": [],
      "source": [
        "embed_dims = 64\n",
        "epochs = 50\n",
        "units = 512"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder():\n",
        "  def __init__(self, input_vocab_size, embedding_dims, units):\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = layers.Embedding(input_vocab_size,embedding_dims)\n",
        "    self.gru_layer = layers.GRU(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedding = self.embedding(inputs)\n",
        "    encoder = self.gru_layer(embedding)\n",
        "\n",
        "    return encoder"
      ],
      "metadata": {
        "id": "fc2nYdHM0Xv0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.w1 = layers.Dense(units, use_bias=True) \n",
        "    self.w2 = layers.Dense(units, use_bias=True) \n",
        "    self.fd = layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.fd(tf.nn.tanh(\n",
        "        self.w1(query_with_time_axis) + self.w2(values)))\n",
        "\n",
        "    attention_weights = softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "TFONKQDzUdmw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder():\n",
        "  def __init__(self, output_vocab_size, embedding_dims, units):\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = layers.Embedding(output_vocab_size, embedding_dims)\n",
        "    self.gru_layer = layers.GRU(self.units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "    self.dense1 = layers.Dense(self.units, activation=tanh)\n",
        "    self.dense2 = layers.Dense(output_vocab_size)\n",
        "\n",
        "  def call(self, inputs, en_outpus, state):\n",
        "    embedding = self.embedding(inputs)\n",
        "    dec_outputs, dec_state = self.gru_layer(embedding, initial_state=state)\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=dec_outputs, values=en_outpus)\n",
        "    \n",
        "    context_and_rnn_output = tf.concat([context_vector, dec_outputs], axis=-1)\n",
        "\n",
        "    attention_vector = self.dense1(context_and_rnn_output)\n",
        "    outputs = self.dense2(attention_vector)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "8ayl_3A_1-IT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_vocab_size, embed_dims, units)\n",
        "en_outputs, en_states = encoder.call(input_batch)\n",
        "\n",
        "en_outputs.shape, en_states.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kio22M03Yp9S",
        "outputId": "b53c47a7-e876-4259-add0-5795f4edcef1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 35, 512]), TensorShape([64, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(target_vocab_size, embed_dims, units)\n",
        "dec_outputs = decoder.call(target_batch, en_outputs, en_states)\n",
        "\n",
        "dec_outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_7A5-GTadcM",
        "outputId": "47012adf-3d18-4afb-9d7a-01f776403235"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 35, 6865])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-4\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "       from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "OdJEFRmxg0zW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatorModel():\n",
        "  def __init__(self, input_vocab_size, \n",
        "               target_vocab_size, \n",
        "               embed_dims, \n",
        "               units):\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.embed_dims = embed_dims\n",
        "    self.units = units\n",
        "\n",
        "    self.encoder = Encoder(self.input_vocab_size, self.embed_dims, self.units)\n",
        "    self.decoder = Decoder(self.target_vocab_size, self.embed_dims, self.units)\n",
        "  \n",
        "  def build_model(self):\n",
        "    en_inputs = layers.Input(shape=(None,))\n",
        "\n",
        "    en_output, en_state = self.encoder.call(en_inputs)\n",
        "\n",
        "    dec_outputs = self.decoder.call(en_inputs, en_output, en_state)\n",
        "\n",
        "    model = Model(inputs=[en_inputs], \n",
        "                  outputs=[dec_outputs])\n",
        "    return model"
      ],
      "metadata": {
        "id": "-ifmblhR4X8i"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TranslatorModel(\n",
        "    input_vocab_size, \n",
        "    target_vocab_size, \n",
        "    embed_dims, \n",
        "    units, \n",
        ").build_model()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SsTiGR8daD4K",
        "outputId": "41b43f97-bd28-4b54-ba86-c88192eb9557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_6 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_14 (Embedding)       (None, None, 64)     392896      ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_15 (Embedding)       (None, None, 64)     439360      ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " gru_14 (GRU)                   [(None, None, 512),  887808      ['embedding_14[0][0]']           \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " gru_15 (GRU)                   [(None, None, 512),  887808      ['embedding_15[0][0]',           \n",
            "                                 (None, 512)]                     'gru_14[0][1]']                 \n",
            "                                                                                                  \n",
            " bahdanau_attention_7 (Bahdanau  ((None, None, 512),  525825     ['gru_15[0][0]',                 \n",
            " Attention)                      (None, None, None,               'gru_14[0][0]']                 \n",
            "                                 1))                                                              \n",
            "                                                                                                  \n",
            " tf.concat_4 (TFOpLambda)       (None, None, 1024)   0           ['bahdanau_attention_7[0][0]',   \n",
            "                                                                  'gru_15[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, None, 512)    524800      ['tf.concat_4[0][0]']            \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, None, 6865)   3521745     ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,180,242\n",
            "Trainable params: 7,180,242\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/translate/checkpoint/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "callback_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3, \n",
        "    verbose=1)\n",
        "\n",
        "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    monitor='val_loss', \n",
        "    verbose=1, \n",
        "    save_weights_only=True, \n",
        "    save_best_only=True)\n",
        "\n",
        "callbacks = [callback_early_stopping,\n",
        "             callback_checkpoint]"
      ],
      "metadata": {
        "id": "VOUD1xKgIDH3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset,\n",
        "          epochs=epochs,\n",
        "          steps_per_epoch=steps_per_epoch,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daAENyOqwdjZ",
        "outputId": "6f0e1fe5-2a5a-4b16-853a-d6f75abb1c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "137/137 [==============================] - ETA: 0s - loss: 2.1227 - accuracy: 0.8093WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "137/137 [==============================] - 294s 2s/step - loss: 2.1227 - accuracy: 0.8093\n",
            "Epoch 2/50\n",
            "  2/137 [..............................] - ETA: 4:58 - loss: 1.1646 - accuracy: 0.8283"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGxWYuiQPnv"
      },
      "outputs": [],
      "source": [
        "# # if use colab\n",
        "# saved_model_path = \"/content/drive/MyDrive/translate/saved_model/translate.h5\"\n",
        "\n",
        "# # if use local env\n",
        "# # saved_model_path = \"code/translate sentence/saved_model/translate.h5\"\n",
        "# saved_model_dir = os.path.dirname(saved_model_path)\n",
        "\n",
        "# if os.path.exists(saved_model_dir):\n",
        "#   shutil.rmtree(saved_model_dir)\n",
        "# model.save(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YmcvYUyzqLd"
      },
      "outputs": [],
      "source": [
        "class Translator:\n",
        "  def __init__(self, modelpath, input_tokenizer, target_tokenizer, maxlen):\n",
        "    self.saved_model_path = modelpath\n",
        "    self.input_tokenizer = input_tokenizer\n",
        "    self.target_tokenizer = target_tokenizer\n",
        "    self.maxlen = maxlen\n",
        "    self._load_model()\n",
        "\n",
        "  def _load_model(self):\n",
        "    self.model = tf.keras.models.load_model(self.saved_model_path)\n",
        "\n",
        "  def translate(self, sentence):\n",
        "    my_model = self.model\n",
        "\n",
        "    target_index_to_word = self.target_tokenizer.index_word\n",
        "    target_index_to_word[0] = '<OOV>'\n",
        "\n",
        "    sequences = self.input_tokenizer.texts_to_sequences([sentence])\n",
        "    sequences = pad_sequences(sequences, maxlen=self.maxlen, padding='post')\n",
        "\n",
        "    predictions = my_model.predict(sequences)\n",
        "\n",
        "    result = self.target_tokenizer.sequences_to_texts(predictions[0])\n",
        "    return result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "translator.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "765233bfe060b87a8be23ec8f17030d468ac6435ae34b0ad14370b4cb734ac81"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}