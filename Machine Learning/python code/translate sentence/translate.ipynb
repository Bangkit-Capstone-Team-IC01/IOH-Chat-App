{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translate",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AxhXmDOQApMo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense,AdditiveAttention,StringLookup,TextVectorization\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open(\"/content/drive/MyDrive/data/eng-ind.csv\") as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  next(csv_reader)\n",
        "\n",
        "  input_data = list()\n",
        "  target_data = list()\n",
        "\n",
        "  for row in csv_reader:\n",
        "    input_data.append(row[0])\n",
        "    target_data.append(row[1])"
      ],
      "metadata": {
        "id": "4ID8ZvSWBDvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87829ca-1c2b-4b76-fa33-e85526e5fa27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_data)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "7QgEbE8DEeFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in dataset.take(1):\n",
        "  print(input_batch,\"\\n\")\n",
        "  print(target_batch)\n",
        "  break"
      ],
      "metadata": {
        "id": "NYgVVwDuIvL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "aQhNbdwIQHGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor = TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "metadata": {
        "id": "_FtgXEKxPjrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_processor.adapt(input_data)\n",
        "output_text_processor.adapt(target_data)"
      ],
      "metadata": {
        "id": "iuAIbJj1Q8M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ala ala aja ni\n",
        "inputs = tf.keras.Input(shape=(3,))\n",
        "x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
        "outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
        "\n",
        "vocab_size = 5000\n",
        "enc_units = 10\n",
        "embedding_dim = 64\n",
        "\n",
        "# encoding layer\n",
        "embedding_enc = Embedding(vocab_size, embedding_dim)(inputs)\n",
        "output_enc, state_enc = GRU(\n",
        "    enc_units,return_sequences=True,\n",
        "    return_state=True,\n",
        "    recurrent_initializer='glorot_uniform')(embedding_enc)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "X_65I-SVRoTL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "vi3APozWR9ty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b9d3e5-2ebf-4c64-bad0-05cd8cb35740"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 3)]               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 16        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41\n",
            "Trainable params: 41\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}