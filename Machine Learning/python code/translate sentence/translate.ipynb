{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translate",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AxhXmDOQApMo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense,AdditiveAttention,StringLookup,TextVectorization\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open(\"/content/drive/MyDrive/data/eng-ind.csv\") as f:\n",
        "  csv_reader = csv.reader(f)\n",
        "  next(csv_reader)\n",
        "\n",
        "  input_data = list()\n",
        "  target_data = list()\n",
        "\n",
        "  for row in csv_reader:\n",
        "    input_data.append(row[0])\n",
        "    target_data.append(row[1])"
      ],
      "metadata": {
        "id": "4ID8ZvSWBDvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a87829ca-1c2b-4b76-fa33-e85526e5fa27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_data)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "7QgEbE8DEeFG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in dataset.take(1):\n",
        "  print(input_batch,\"\\n\")\n",
        "  print(target_batch)\n",
        "  break"
      ],
      "metadata": {
        "id": "NYgVVwDuIvL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "aQhNbdwIQHGU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor = TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "metadata": {
        "id": "_FtgXEKxPjrj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_processor.adapt(input_data)\n",
        "output_text_processor.adapt(target_data)"
      ],
      "metadata": {
        "id": "iuAIbJj1Q8M3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X_65I-SVRoTL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vi3APozWR9ty"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}