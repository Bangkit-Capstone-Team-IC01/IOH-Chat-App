{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsU8eTWe2s-M",
        "outputId": "f4a6e0ce-1f23-41e2-e6ce-6ed64bd0af1d"
      },
      "outputs": [],
      "source": [
        "#@title Load Imports\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string, re\n",
        "import nltk\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras_preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q8sc7VUS0bla"
      },
      "outputs": [],
      "source": [
        "#@title Clone Git\n",
        "\n",
        "git_dir = '/content/IOH-Chat-App'\n",
        "git_url = 'https://github.com/bangkit-team/IOH-chat-app.git'\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call(['git', 'clone', git_url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYdPSeMdwaiv",
        "outputId": "072600db-a3b4-4201-8181-bfbdf53efc10"
      },
      "outputs": [],
      "source": [
        "#@title Connect with Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OHAnj6apAsf"
      },
      "outputs": [],
      "source": [
        "SENTIMENT_CSV = '/content/IOH-chat-app/MachineLearning/datasets/feedbacks/data_feedbacks.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQrToAkZ8tP8"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(ulasan):\n",
        "    stopwords = ['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia', 'seperti', \n",
        "              'jika', 'sehingga', 'kembali', 'dan', 'karena', 'kepada', 'oleh', 'saat', 'harus', 'sementara', \n",
        "              'setelah', 'belum', 'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', \n",
        "              'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bisa', 'bahwa', 'atau', 'hanya', 'kita', 'dengan', 'akan', \n",
        "              'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', 'anda', 'begitu', 'mengapa', \n",
        "              'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana',\n",
        "              'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi',\n",
        "              'sementara', 'tetapi', 'apakah', 'kecuali', 'sebab', 'selain', 'seolah', 'seterusnya', 'tanpa', 'agak',\n",
        "              'boleh', 'dapat', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'ingin', 'juga',\n",
        "              'nggak', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya',\n",
        "              'sesuatu', 'pasti', 'saja', 'toh', 'walau', 'tolong', 'tentu', 'amat', 'apalagi', 'bagaimanapun',\n",
        "              'dengan', 'ia', 'bahwa', 'oleh']\n",
        "\n",
        "    ulasan = str(ulasan).lower()\n",
        "    \n",
        "    words = ulasan.split()\n",
        "    ulasan = []\n",
        "    for r in words:\n",
        "        if not r in stopwords:\n",
        "            ulasan.append(r)\n",
        "            \n",
        "    ulasan=' '.join(ulasan)\n",
        "    return ulasan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuztjVYs5F1b"
      },
      "outputs": [],
      "source": [
        "def parse_data_from_file(filename):\n",
        "    ulasan = []\n",
        "    label = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        skip = True\n",
        "        if skip:\n",
        "            skip = False\n",
        "            reader = csv.reader(csvfile, delimiter=',')\n",
        "            next(reader)\n",
        "\n",
        "            for row in reader:\n",
        "                row[0] = remove_stopwords(row[0])\n",
        "                row[1] = remove_stopwords(row[1])\n",
        "                label.append(row[0])\n",
        "                ulasan.append(row[1])\n",
        "            \n",
        "    return ulasan, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGqKK8ss5Qr5",
        "outputId": "e432eb75-e27c-484b-c5c9-8f5b70187d71"
      },
      "outputs": [],
      "source": [
        "ulasan, label = parse_data_from_file(SENTIMENT_CSV)\n",
        "\n",
        "print(f'Dataset contains {len(ulasan)} examples\\n')\n",
        "print(f'Example 1:\\nText: {ulasan[0]}\\nLabel :{label[0]}\\n')\n",
        "print(f'Example 2:\\nText: {ulasan[1]}\\nLabel :{label[1]}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYezrSbIzaD_"
      },
      "outputs": [],
      "source": [
        "def preprocess(ulasan):\n",
        "    ulasan = ulasan.lower() \n",
        "    ulasan = ulasan.strip()  \n",
        "    ulasan = re.compile('<.*?>').sub('', ulasan) \n",
        "    ulasan = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', ulasan)  \n",
        "    ulasan = re.sub('\\s+', ' ', ulasan)  \n",
        "    ulasan = re.sub(r'\\[*\\]',' ',ulasan) \n",
        "    ulasan = re.sub(r'[^\\w\\s]', '', str(ulasan).lower().strip())\n",
        "    ulasan = re.sub(r'\\d',' ',ulasan) \n",
        "    ulasan = re.sub(r'\\s+',' ',ulasan) \n",
        "    return ulasan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw-bJuDABnaT"
      },
      "outputs": [],
      "source": [
        "wl = WordNetLemmatizer()\n",
        " \n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def lemmatizer(ulasan):\n",
        "    word_pos_tags = nltk.pos_tag(word_tokenize(ulasan))\n",
        "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)]\n",
        "    return ' '.join(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z3Vf2WF08vi"
      },
      "outputs": [],
      "source": [
        "def finalpreprocess(ulasan):\n",
        "    return lemmatizer(remove_stopwords(preprocess(ulasan)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "bcNVwXY4rfqW",
        "outputId": "6a28e4a5-cc98-49ae-bfc0-f23580626dd6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(SENTIMENT_CSV)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "aCUKd1a03sh-",
        "outputId": "7a7fe4ae-37d1-469c-955b-282a7704c27c"
      },
      "outputs": [],
      "source": [
        "clean_text = df['ulasan'].apply(lambda x: finalpreprocess(x))\n",
        "\n",
        "label_column = df.pop('label')\n",
        "df.insert(1, 'label', label_column)\n",
        "\n",
        "df['ulasan'] = clean_text\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "ogSQCk4rshSe",
        "outputId": "cc13a9b0-36ae-4357-d48b-1afff42e9006"
      },
      "outputs": [],
      "source": [
        "label_count = df['label'].value_counts()\n",
        "\n",
        "for i in range(0,2):\n",
        "    print(f'Class {i} : {label_count[i]}')\n",
        "\n",
        "label_count.plot(kind='bar', title='Count label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "IAZSajBSszxI",
        "outputId": "6a031808-4920-411e-ade1-507a5cabbc62"
      },
      "outputs": [],
      "source": [
        "df_class_0 = df[df['label'] == 0]\n",
        "df_class_1 = df[df['label'] == 1]\n",
        "\n",
        "df_class_1_over = df_class_1.sample(label_count[0], replace=True)\n",
        "\n",
        "df_over = pd.concat([\n",
        "                    df_class_0,\n",
        "                    df_class_1_over], axis=0)\n",
        "\n",
        "print('Random Over Sampling')\n",
        "print(df_over['label'].value_counts())\n",
        "df_over['label'].value_counts().plot(kind='bar', title='Count label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu1tZvNj1owF"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 64\n",
        "TRUNCATING = 'post'\n",
        "PADDING = 'post'\n",
        "OOV_TOKEN = '<OOV>'\n",
        "MAX_EXAMPLES = 16000\n",
        "TESTING_SPLIT = 0.2\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNOzxdX52mdx"
      },
      "outputs": [],
      "source": [
        "x = df.ulasan.values\n",
        "y = df.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZwUH9emEwug",
        "outputId": "aca40ed5-ab0f-4429-a2b7-568cddf42262"
      },
      "outputs": [],
      "source": [
        "MAXLEN = max([len(i) for i in x])\n",
        "MAXLEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJg-wFcT5vWE"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, num_words, oov_token):\n",
        "  tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "  tokenizer.fit_on_texts(text)\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf3zdquX5_Ik"
      },
      "outputs": [],
      "source": [
        "def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n",
        "  sequences = tokenizer.texts_to_sequences(sentences)\n",
        "  pad_seqs = pad_sequences(\n",
        "      sequences, padding=padding, truncating=truncating, maxlen=maxlen)\n",
        "\n",
        "  return pad_seqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T43qtrX1s_W",
        "outputId": "e4eb042f-5385-43a9-a89e-5ff6b9b5c2f7"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=TESTING_SPLIT, random_state=1)\n",
        "\n",
        "print(f'There are {len(x_train)} ulasan for training.')\n",
        "print(f'There are {len(y_train)} label for training.')\n",
        "print(f'There are {len(x_test)} ulasan for testing.')\n",
        "print(f'There are {len(y_test)} label for testing.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pUBalOT1eOJ",
        "outputId": "01600e0f-e310-4371-8e84-0946f2eb31f9"
      },
      "outputs": [],
      "source": [
        "tokenizer = tokenize(x_train, MAX_EXAMPLES, OOV_TOKEN)\n",
        "\n",
        "index_word = tokenizer.index_word\n",
        "VOCAB_SIZE = len(index_word)\n",
        "\n",
        "print(f'Vocabulary contains {VOCAB_SIZE} words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2J2Ciky-zxY"
      },
      "outputs": [],
      "source": [
        "tokenizer_json_dir = '/content/drive/MyDrive/Company Case Bangkit/FeedbacksModel/vocab.json'\n",
        "\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open(tokenizer_json_dir, 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHb8NTsB8FWx",
        "outputId": "e30d4d05-8350-467a-b259-726986d738a8"
      },
      "outputs": [],
      "source": [
        "x_train_padded = seq_pad_and_trunc(x_train, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "x_test_padded = seq_pad_and_trunc(x_test, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "\n",
        "print(f'Padded and truncated training sequences have shape: {x_train_padded.shape}')\n",
        "print(f'Padded and truncated testing sequences have shape: {x_test_padded.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDMpq3AjlHSt"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, embedding_dim, metrics):\n",
        "  model = tf.keras.Sequential([ \n",
        "    tf.keras.layers.Embedding(vocab_size + 1, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "  ])\n",
        "  \n",
        "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                metrics=metrics) \n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XirV-6eylTOB",
        "outputId": "7436ef16-765a-42ed-f8b5-d4b476ca57df"
      },
      "outputs": [],
      "source": [
        "threshold = 0.35\n",
        "\n",
        "METRICS = [\n",
        "    tf.keras.metrics.BinaryAccuracy(threshold=threshold),\n",
        "    tf.keras.metrics.Precision(thresholds=threshold),\n",
        "    tf.keras.metrics.Recall(thresholds=threshold),\n",
        "]\n",
        "\n",
        "model = create_model(VOCAB_SIZE, EMBEDDING_DIM, METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCzZZ2sW06s-",
        "outputId": "bce57f44-75b2-49a6-e180-0ad62eb349a3"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    x_train_padded, y_train, epochs=15, validation_data=(\n",
        "        x_test_padded, y_test),verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVhyhziywTcU",
        "outputId": "b3a0c6b8-e496-4383-bb0a-bb6205c98661"
      },
      "outputs": [],
      "source": [
        "export_dir = '/content/drive/MyDrive/Company Case Bangkit/FeedbacksModel/saved_model'\n",
        "\n",
        "if os.path.exists(export_dir):\n",
        "  shutil.rmtree(export_dir)\n",
        "\n",
        "model.save(export_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYQJTs7wA8dy"
      },
      "outputs": [],
      "source": [
        "class FeedbackPredict:\n",
        "  def __init__(self, model_path, tokenier_json_path, maxlen=309):\n",
        "    self.model_path = model_path\n",
        "    self.tokenier_json_path = tokenier_json_path\n",
        "    self.padding = 'post'\n",
        "    self.truncating = 'post'\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "    self._load_model()\n",
        "    self._load_tokenizer()\n",
        "\n",
        "  def _load_model(self):\n",
        "    self.model = tf.keras.models.load_model(self.model_path, compile=True)\n",
        "  \n",
        "  def _load_tokenizer(self):\n",
        "    with open(self.tokenier_json_path) as f:\n",
        "      data = json.load(f)\n",
        "      self.tokenizer = tokenizer_from_json(data)\n",
        "\n",
        "  def __call__(self, feedback):\n",
        "    sequences = self.tokenizer.texts_to_sequences([feedback])\n",
        "    pad_seqs = pad_sequences(sequences, \n",
        "                             padding=self.padding, \n",
        "                             truncating=self.truncating, \n",
        "                             maxlen=self.maxlen)\n",
        "    \n",
        "    prediction = self.model.predict(pad_seqs)[0][0]\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WxhlgjTEf7c",
        "outputId": "52be31e7-bf00-47db-fea8-1c17ac5bc6a2"
      },
      "outputs": [],
      "source": [
        "exp_pos_text = \"aplikasi ini sangat bagus desainnya menarik, dan fungsionalitasnya dapat berjalan dengan baik\"\n",
        "exp_neg_text = \"jelek banget, chat nya tidak realtime, design nya juga tidak user friendly\"\n",
        "\n",
        "feedback_predict =  FeedbackPredict(export_dir, tokenizer_json_dir)\n",
        "y_predict = feedback_predict(exp_neg_text)\n",
        "\n",
        "print(y_predict)\n",
        "\n",
        "if y_predict >= threshold:\n",
        "  print('Positive')\n",
        "else:\n",
        "  print('Negative')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "model_review.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
