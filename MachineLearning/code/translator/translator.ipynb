{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF8TKqYD-H8W",
        "outputId": "c1669a2c-c6b2-47e4-d7fe-bf77e1a1c354"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.7/dist-packages (3.18.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: tensorflow-hub<0.13,>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Requirement already satisfied: packaging~=20.9 in /usr/local/lib/python3.7/dist-packages (from tensorflowjs) (20.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (4.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.46.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.5.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.21.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.17.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.26.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (14.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<3,>=2.1.0->tensorflowjs) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "56aTseK4q9ol"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "from google.colab import drive, files #if use colab\n",
        "from tensorflow.nn import relu, tanh, softmax\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz4aY7oDbyeg",
        "outputId": "e786ebcd-26b5-40ab-e512-571338e8abc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# if use colab\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2ZtMplOQv6Lz"
      },
      "outputs": [],
      "source": [
        "#if use colab\n",
        "git_dir = \"/content/IOH-Chat-App\"\n",
        "git_url = \"https://github.com/bangkit-team/IOH-chat-app.git\"\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call([\"git\", \"clone\", git_url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pN2M4PirLEQB"
      },
      "outputs": [],
      "source": [
        "filedir1 = \"/content/IOH-chat-app/MachineLearning/datasets/translation/result/eng-ind.csv\" # #if use colab\n",
        "# filedir2 = \"/content/IOH-chat-app/MachineLearning/datasets/spam/emails.csv\" # #if use colab\n",
        "# filedir = \"../../datasets/translate sentence/result/eng-ind.csv\" #if use local env\n",
        "# filedir = \"../../datasets/spam/emails.csv\" #if use local env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(filedir1)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CTSHYGqdTyPk",
        "outputId": "c744e97a-30a5-42a8-ef42-faba324a0b5a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                English  \\\n",
              "0                                                  Run!   \n",
              "1                                                  Who?   \n",
              "2                                                  Wow!   \n",
              "3                                                 Help!   \n",
              "4                                                 Jump!   \n",
              "...                                                 ...   \n",
              "8814  Every student who has graduated from our unive...   \n",
              "8815  If you don't want to put on sunscreen, that's ...   \n",
              "8816  When she was finished ironing, Mary switched o...   \n",
              "8817  Irene Pepperberg, a researcher at Northwestern...   \n",
              "8818  If a person has not had a chance to acquire hi...   \n",
              "\n",
              "                                              Indonesia  \n",
              "0                                                 Lari!  \n",
              "1                                                Siapa?  \n",
              "2                                                  Wow!  \n",
              "3                                               Tolong!  \n",
              "4                                               Lompat!  \n",
              "...                                                 ...  \n",
              "8814  Semua mahasiswa yang telah menyelesaikan studi...  \n",
              "8815  Kalau kamu tidak mau pakai tabir surya, ya, te...  \n",
              "8816  Ketika dia sudah selesai menyetrika, Mary mema...  \n",
              "8817  Irene Pepperberg, seorang peneliti di Universi...  \n",
              "8818  Jika seseorang tidak berkesempatan untuk mengu...  \n",
              "\n",
              "[8819 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3debef0e-d68c-4257-9765-7c070f395621\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Indonesia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Lari!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who?</td>\n",
              "      <td>Siapa?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Wow!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Help!</td>\n",
              "      <td>Tolong!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jump!</td>\n",
              "      <td>Lompat!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8814</th>\n",
              "      <td>Every student who has graduated from our unive...</td>\n",
              "      <td>Semua mahasiswa yang telah menyelesaikan studi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8815</th>\n",
              "      <td>If you don't want to put on sunscreen, that's ...</td>\n",
              "      <td>Kalau kamu tidak mau pakai tabir surya, ya, te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8816</th>\n",
              "      <td>When she was finished ironing, Mary switched o...</td>\n",
              "      <td>Ketika dia sudah selesai menyetrika, Mary mema...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8817</th>\n",
              "      <td>Irene Pepperberg, a researcher at Northwestern...</td>\n",
              "      <td>Irene Pepperberg, seorang peneliti di Universi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8818</th>\n",
              "      <td>If a person has not had a chance to acquire hi...</td>\n",
              "      <td>Jika seseorang tidak berkesempatan untuk mengu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8819 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3debef0e-d68c-4257-9765-7c070f395621')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3debef0e-d68c-4257-9765-7c070f395621 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3debef0e-d68c-4257-9765-7c070f395621');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df2 = pd.read_csv(filedir2)\n",
        "# df2 = df2.rename(columns={\"text\": \"English\", \"teks\": \"Indonesia\"})\n",
        "# df2 = df2.drop(\"spam\", axis=1)\n",
        "# df2"
      ],
      "metadata": {
        "id": "-eQatoc7bG2r"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df2_len = len(df2)\n",
        "# df = pd.concat([df1, df2[:df2_len//2]])\n",
        "# df"
      ],
      "metadata": {
        "id": "VLgct1pkbhn4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RgUUgmronChh"
      },
      "outputs": [],
      "source": [
        "start_mark = '<start>'\n",
        "end_mark = '<end>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KA5VqxGKtrWx"
      },
      "outputs": [],
      "source": [
        "class TranslatorDataset:\n",
        "  def __init__(self, dataframe):\n",
        "    self.dataframe = dataframe\n",
        "    self.input_tokenizer = None\n",
        "    self.target_tokenizer = None\n",
        "    self._load_data_from_file()\n",
        "\n",
        "  def _load_data_from_file(self):\n",
        "    df = self.dataframe\n",
        "\n",
        "    input_lang = df.English.values\n",
        "    target_lang = df.Indonesia.values\n",
        "\n",
        "    return input_lang, target_lang\n",
        "\n",
        "  def _normalize_and_preprocess(self, text, use_mark=False):\n",
        "    if use_mark:\n",
        "      text = text.lower().strip()\n",
        "      text = \" \".join([start_mark, text, end_mark])\n",
        "    else:\n",
        "      text = text.lower().strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "  def _tokenize(self, sentences, num_words, maxlen):\n",
        "    punctuation = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=num_words, filters=punctuation, lower=False)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    sequences = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
        "\n",
        "    return sequences, tokenizer\n",
        "\n",
        "  def _create_dataset(self):\n",
        "    input_lang, target_lang = self._load_data_from_file()\n",
        "\n",
        "    input_sentence = np.array(\n",
        "        list(map(lambda x: self._normalize_and_preprocess(x, False), input_lang)))\n",
        "    \n",
        "    target_sentence = np.array(\n",
        "        list(map(lambda y: self._normalize_and_preprocess(y, True), target_lang)))\n",
        "    \n",
        "    return input_sentence, target_sentence\n",
        "\n",
        "  def _load_dataset(self, num_words):\n",
        "    input_lang, target_lang = self._create_dataset()\n",
        "\n",
        "    self.maxlen = max([len(i)for i in input_lang]) // 5\n",
        "    self.buffer_size = len(input_lang)\n",
        "\n",
        "    input_sequences, input_tokenizer = self._tokenize(\n",
        "        input_lang, num_words, self.maxlen)\n",
        "    \n",
        "    target_sequences, target_tokenizer = self._tokenize(\n",
        "        target_lang, num_words, self.maxlen,)\n",
        "\n",
        "    return (input_sequences, input_tokenizer), (target_sequences, target_tokenizer)\n",
        "  \n",
        "  def get(self, num_words, batch_size):\n",
        "    input, target = self._load_dataset(num_words)\n",
        "\n",
        "    input_sequences, self.input_tokenizer = input\n",
        "    target_sequences, self.target_tokenizer = target\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "    dataset = dataset.shuffle(self.buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    return self.input_tokenizer, self.target_tokenizer, dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8lYoZZjqXF__"
      },
      "outputs": [],
      "source": [
        "num_words = 8000\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QW7fD1GdrExy"
      },
      "outputs": [],
      "source": [
        "translator_dataset = TranslatorDataset(df)\n",
        "input_tokenizer, target_tokenizer, dataset = translator_dataset.get(num_words, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3lQitbaJXt4O"
      },
      "outputs": [],
      "source": [
        "input_batch, target_batch = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j91LroJX50x",
        "outputId": "d94e88fe-331a-4979-c2e8-4c944562c111"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 32]), TensorShape([64, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "input_batch.shape, target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI-TplUE22Tk",
        "outputId": "194058e0-59c7-4477-f917-581cfdba7969"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 4091, 4876)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "input_vocab_size = len(input_tokenizer.index_word) + 1\n",
        "target_vocab_size = len(target_tokenizer.index_word) + 1\n",
        "input_maxlen = input_batch.shape[1]\n",
        "target_maxlen = target_batch.shape[1]\n",
        "\n",
        "input_maxlen, target_maxlen, input_vocab_size, target_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_wi_json = \"input_word_index.json\"\n",
        "\n",
        "with open(input_wi_json, 'w') as f:\n",
        "    json.dump(input_tokenizer.word_index, f)\n",
        "\n",
        "files.download(input_wi_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fuPKVVQOFP9z",
        "outputId": "b057cfb0-54a8-4338-80fb-913d10778d22"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1d0922d5-3b51-4ad4-994d-b8f1b2a0815a\", \"input_word_index.json\", 66138)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_iw_json = \"target_index_word.json\"\n",
        "\n",
        "with open(target_iw_json, 'w') as f:\n",
        "    json.dump(target_tokenizer.index_word, f)\n",
        "\n",
        "files.download(target_iw_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OHS4eHGBFgLg",
        "outputId": "5429991c-7a74-453f-c885-381e611fbfe8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_803fc75f-dd05-4a42-9fe7-b23267715acc\", \"target_index_word.json\", 94118)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPp4UAxdxemM",
        "outputId": "451d5325-b071-4ddd-a1bb-a45de63981be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              "array([  52,   22,    1, 2467,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "input_example = input_batch[-1]\n",
        "input_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r5qpp8Hxjzk",
        "outputId": "b1ac89e1-a2bf-42b5-f19c-64457c58fcba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              "array([  1,   7, 378,  18,  37,   2,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "target_example = target_batch[-1]\n",
        "target_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulo132GvOX0l",
        "outputId": "c1019a80-5d31-49af-b182-cf5e2d3c8c8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['where are you bound']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "input_sentence = input_tokenizer.sequences_to_texts([input_example.numpy()])\n",
        "input_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X59pOM0qOtyb",
        "outputId": "4cbd3fb7-2523-4f15-d443-3208141fb9fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> kamu menuju ke mana <end>']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "target_sentence = target_tokenizer.sequences_to_texts([target_example.numpy()])\n",
        "target_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "outputs": [],
      "source": [
        "embed_dims = 128\n",
        "units = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "fc2nYdHM0Xv0"
      },
      "outputs": [],
      "source": [
        "class Encoder(layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dims, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.embedding_dims = embedding_dims\n",
        "\n",
        "    self.embedding = layers.Embedding(self.input_vocab_size, self.embedding_dims)\n",
        "    self.lstm_layer = layers.LSTM(self.units,\n",
        "                                 return_sequences=True,\n",
        "                                 return_state=True,\n",
        "                                 recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedding = self.embedding(inputs)\n",
        "    encoder = self.lstm_layer(embedding, initial_state=None)\n",
        "\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "TFONKQDzUdmw"
      },
      "outputs": [],
      "source": [
        "# class BahdanauAttention(layers.Layer):\n",
        "#   def __init__(self, units):\n",
        "#     super(BahdanauAttention, self).__init__()\n",
        "#     self.w1 = layers.Dense(units, use_bias=True) \n",
        "#     self.w2 = layers.Dense(units, use_bias=True) \n",
        "#     self.fd = layers.Dense(1)\n",
        "\n",
        "#   def call(self, query, values):\n",
        "#     query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "#     score = self.fd(tf.nn.tanh(\n",
        "#         self.w1(query_with_time_axis) + self.w2(values)))\n",
        "\n",
        "#     attention_weights = softmax(score, axis=1)\n",
        "\n",
        "#     context_vector = attention_weights * values\n",
        "#     context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "#     return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "8ayl_3A_1-IT"
      },
      "outputs": [],
      "source": [
        "class Decoder(layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dims, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.units = units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dims = embedding_dims\n",
        "\n",
        "    self.embedding = layers.Embedding(self.output_vocab_size, self.embedding_dims)\n",
        "    self.lstm_layer = layers.LSTM(self.units,\n",
        "                                 return_sequences=True,\n",
        "                                 return_state=True,\n",
        "                                 recurrent_initializer='glorot_uniform')\n",
        "    self.attention = layers.AdditiveAttention()\n",
        "    self.dense1 = layers.Dense(self.units, activation=tanh)\n",
        "    self.dense2 = layers.Dense(self.output_vocab_size)\n",
        "\n",
        "  def call(self, inputs, en_outputs, state):\n",
        "    embedding = self.embedding(inputs)\n",
        "    dec_outputs, dec_h_state, dec_c_state = self.lstm_layer(embedding, initial_state=state)\n",
        "    query_value_attention_seq = self.attention([dec_outputs, en_outputs])\n",
        "\n",
        "    attention_vector = self.dense1(query_value_attention_seq)\n",
        "    outputs = self.dense2(attention_vector)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kio22M03Yp9S",
        "outputId": "e050c458-ed96-49f3-b6de-474d61fb276d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 32, 512]), TensorShape([64, 512]), TensorShape([64, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "encoder = Encoder(input_vocab_size, embed_dims, units)\n",
        "en_outputs, en_h_state, en_c_state = encoder(input_batch)\n",
        "\n",
        "en_outputs.shape, en_h_state.shape, en_c_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_7A5-GTadcM",
        "outputId": "49b6d7ae-62ff-4f4c-9762-cb7a2b9fc556"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 32, 4876])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "decoder = Decoder(target_vocab_size, embed_dims, units)\n",
        "dec_outputs= decoder(target_batch, en_outputs, [en_h_state, en_c_state])\n",
        "\n",
        "dec_outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "OdJEFRmxg0zW"
      },
      "outputs": [],
      "source": [
        "lr = 1e-3\n",
        "epochs = 30\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "       from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "-ifmblhR4X8i"
      },
      "outputs": [],
      "source": [
        "class TranslatorModel:\n",
        "  def __init__(self, input_vocab_size, \n",
        "               target_vocab_size, \n",
        "               embed_dims, \n",
        "               units):\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.embed_dims = embed_dims\n",
        "    self.units = units\n",
        "\n",
        "    self.encoder = Encoder(self.input_vocab_size, self.embed_dims, self.units)\n",
        "    self.decoder = Decoder(self.target_vocab_size, self.embed_dims, self.units)\n",
        "\n",
        "  def get_encoder(self):\n",
        "    return self.encoder\n",
        "\n",
        "  def get_decoder(self):\n",
        "    return self.decoder\n",
        "  \n",
        "  def build_model(self):\n",
        "    en_inputs = layers.Input(shape=(None,))\n",
        "    en_output, en_h_state, en_c_state = self.encoder.call(en_inputs)\n",
        "\n",
        "    dec_outputs = self.decoder.call(en_inputs, en_output, [en_h_state, en_c_state])\n",
        "\n",
        "    model = Model(inputs=[en_inputs], \n",
        "                  outputs=[dec_outputs])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "SsTiGR8daD4K"
      },
      "outputs": [],
      "source": [
        "translator_model = TranslatorModel(\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    embed_dims,\n",
        "    units,\n",
        ")\n",
        "model = translator_model.build_model()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOUD1xKgIDH3",
        "outputId": "e0094e1d-ce6b-4408-c2f6-eca2cc8ddf04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_34 (Embedding)       (None, None, 128)    523648      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_35 (Embedding)       (None, None, 128)    624128      ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_34 (LSTM)                 [(None, None, 512),  1312768     ['embedding_34[0][0]']           \n",
            "                                 (None, 512),                                                     \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " lstm_35 (LSTM)                 [(None, None, 512),  1312768     ['embedding_35[0][0]',           \n",
            "                                 (None, 512),                     'lstm_34[0][1]',                \n",
            "                                 (None, 512)]                     'lstm_34[0][2]']                \n",
            "                                                                                                  \n",
            " additive_attention_9 (Additive  (None, None, 512)   512         ['lstm_35[0][0]',                \n",
            " Attention)                                                       'lstm_34[0][0]']                \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, None, 512)    262656      ['additive_attention_9[0][0]']   \n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, None, 4876)   2501388     ['dense_58[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,537,868\n",
            "Trainable params: 6,537,868\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/translate/checkpoint/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "callback_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss',\n",
        "    patience=3, \n",
        "    verbose=1)\n",
        "\n",
        "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    monitor='loss', \n",
        "    verbose=1, \n",
        "    save_freq=10,\n",
        "    save_weights_only=True, \n",
        "    save_best_only=True)\n",
        "\n",
        "callbacks = [callback_early_stopping,\n",
        "             callback_checkpoint]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daAENyOqwdjZ",
        "outputId": "bf545c1b-6124-48e5-b251-0d12556ce1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  8/137 [>.............................] - ETA: 6s - loss: 5.4103 - accuracy: 0.6783\n",
            "Epoch 1: loss improved from inf to 4.75472, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 19/137 [===>..........................] - ETA: 7s - loss: 3.4325 - accuracy: 0.7303\n",
            "Epoch 1: loss improved from 4.75472 to 3.35112, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 29/137 [=====>........................] - ETA: 7s - loss: 2.8611 - accuracy: 0.7454\n",
            "Epoch 1: loss improved from 3.35112 to 2.82394, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 38/137 [=======>......................] - ETA: 7s - loss: 2.5684 - accuracy: 0.7507\n",
            "Epoch 1: loss improved from 2.82394 to 2.51358, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 48/137 [=========>....................] - ETA: 6s - loss: 2.3413 - accuracy: 0.7551\n",
            "Epoch 1: loss improved from 2.51358 to 2.30361, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 58/137 [===========>..................] - ETA: 6s - loss: 2.1866 - accuracy: 0.7579\n",
            "Epoch 1: loss improved from 2.30361 to 2.15988, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 68/137 [=============>................] - ETA: 5s - loss: 2.0767 - accuracy: 0.7596\n",
            "Epoch 1: loss improved from 2.15988 to 2.05684, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 78/137 [================>.............] - ETA: 4s - loss: 1.9906 - accuracy: 0.7606\n",
            "Epoch 1: loss improved from 2.05684 to 1.97572, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 88/137 [==================>...........] - ETA: 3s - loss: 1.9198 - accuracy: 0.7615\n",
            "Epoch 1: loss improved from 1.97572 to 1.90567, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            " 98/137 [====================>.........] - ETA: 3s - loss: 1.8584 - accuracy: 0.7623\n",
            "Epoch 1: loss improved from 1.90567 to 1.84643, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            "108/137 [======================>.......] - ETA: 2s - loss: 1.8048 - accuracy: 0.7635\n",
            "Epoch 1: loss improved from 1.84643 to 1.79588, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            "118/137 [========================>.....] - ETA: 1s - loss: 1.7651 - accuracy: 0.7643\n",
            "Epoch 1: loss improved from 1.79588 to 1.75695, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            "128/137 [===========================>..] - ETA: 0s - loss: 1.7273 - accuracy: 0.7662\n",
            "Epoch 1: loss improved from 1.75695 to 1.72027, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0001.ckpt\n",
            "137/137 [==============================] - 13s 79ms/step - loss: 1.6988 - accuracy: 0.7682\n",
            "Epoch 2/30\n",
            "  1/137 [..............................] - ETA: 6s - loss: 1.2130 - accuracy: 0.8052\n",
            "Epoch 2: loss improved from 1.72027 to 1.27290, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            " 11/137 [=>............................] - ETA: 9s - loss: 1.2823 - accuracy: 0.7932 \n",
            "Epoch 2: loss did not improve from 1.27290\n",
            " 21/137 [===>..........................] - ETA: 6s - loss: 1.2797 - accuracy: 0.7938\n",
            "Epoch 2: loss did not improve from 1.27290\n",
            " 31/137 [=====>........................] - ETA: 5s - loss: 1.2770 - accuracy: 0.7943\n",
            "Epoch 2: loss did not improve from 1.27290\n",
            " 41/137 [=======>......................] - ETA: 4s - loss: 1.2713 - accuracy: 0.7951\n",
            "Epoch 2: loss improved from 1.27290 to 1.26844, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            " 51/137 [==========>...................] - ETA: 4s - loss: 1.2664 - accuracy: 0.7959\n",
            "Epoch 2: loss improved from 1.26844 to 1.26720, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            " 61/137 [============>.................] - ETA: 4s - loss: 1.2647 - accuracy: 0.7960\n",
            "Epoch 2: loss improved from 1.26720 to 1.26312, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            " 71/137 [==============>...............] - ETA: 3s - loss: 1.2637 - accuracy: 0.7960\n",
            "Epoch 2: loss did not improve from 1.26312\n",
            " 81/137 [================>.............] - ETA: 3s - loss: 1.2629 - accuracy: 0.7958\n",
            "Epoch 2: loss improved from 1.26312 to 1.26108, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            " 91/137 [==================>...........] - ETA: 2s - loss: 1.2619 - accuracy: 0.7963\n",
            "Epoch 2: loss did not improve from 1.26108\n",
            "101/137 [=====================>........] - ETA: 2s - loss: 1.2588 - accuracy: 0.7976\n",
            "Epoch 2: loss improved from 1.26108 to 1.25803, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            "111/137 [=======================>......] - ETA: 1s - loss: 1.2554 - accuracy: 0.7994\n",
            "Epoch 2: loss improved from 1.25803 to 1.25533, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            "121/137 [=========================>....] - ETA: 1s - loss: 1.2511 - accuracy: 0.8009\n",
            "Epoch 2: loss improved from 1.25533 to 1.24986, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            "131/137 [===========================>..] - ETA: 0s - loss: 1.2461 - accuracy: 0.8021\n",
            "Epoch 2: loss improved from 1.24986 to 1.24657, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0002.ckpt\n",
            "137/137 [==============================] - 9s 67ms/step - loss: 1.2445 - accuracy: 0.8028\n",
            "Epoch 3/30\n",
            "  5/137 [>.............................] - ETA: 5s - loss: 1.1682 - accuracy: 0.8152\n",
            "Epoch 3: loss improved from 1.24657 to 1.16059, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            " 14/137 [==>...........................] - ETA: 8s - loss: 1.1510 - accuracy: 0.8192\n",
            "Epoch 3: loss improved from 1.16059 to 1.15262, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            " 24/137 [====>.........................] - ETA: 9s - loss: 1.1508 - accuracy: 0.8197 \n",
            "Epoch 3: loss improved from 1.15262 to 1.15173, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            " 34/137 [======>.......................] - ETA: 8s - loss: 1.1472 - accuracy: 0.8208\n",
            "Epoch 3: loss improved from 1.15173 to 1.14638, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            " 44/137 [========>.....................] - ETA: 7s - loss: 1.1512 - accuracy: 0.8206\n",
            "Epoch 3: loss did not improve from 1.14638\n",
            " 54/137 [==========>...................] - ETA: 5s - loss: 1.1478 - accuracy: 0.8213\n",
            "Epoch 3: loss did not improve from 1.14638\n",
            " 64/137 [=============>................] - ETA: 4s - loss: 1.1495 - accuracy: 0.8210\n",
            "Epoch 3: loss did not improve from 1.14638\n",
            " 74/137 [===============>..............] - ETA: 3s - loss: 1.1455 - accuracy: 0.8217\n",
            "Epoch 3: loss improved from 1.14638 to 1.14511, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            " 84/137 [=================>............] - ETA: 3s - loss: 1.1444 - accuracy: 0.8219\n",
            "Epoch 3: loss improved from 1.14511 to 1.14379, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            " 94/137 [===================>..........] - ETA: 2s - loss: 1.1436 - accuracy: 0.8221\n",
            "Epoch 3: loss improved from 1.14379 to 1.14354, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            "104/137 [=====================>........] - ETA: 2s - loss: 1.1422 - accuracy: 0.8223\n",
            "Epoch 3: loss improved from 1.14354 to 1.14086, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            "114/137 [=======================>......] - ETA: 1s - loss: 1.1416 - accuracy: 0.8224\n",
            "Epoch 3: loss did not improve from 1.14086\n",
            "124/137 [==========================>...] - ETA: 0s - loss: 1.1397 - accuracy: 0.8229\n",
            "Epoch 3: loss improved from 1.14086 to 1.13875, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0003.ckpt\n",
            "134/137 [============================>.] - ETA: 0s - loss: 1.1384 - accuracy: 0.8231\n",
            "Epoch 3: loss did not improve from 1.13875\n",
            "137/137 [==============================] - 9s 66ms/step - loss: 1.1388 - accuracy: 0.8231\n",
            "Epoch 4/30\n",
            "  7/137 [>.............................] - ETA: 5s - loss: 1.0919 - accuracy: 0.8266\n",
            "Epoch 4: loss improved from 1.13875 to 1.10719, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0004.ckpt\n",
            " 17/137 [==>...........................] - ETA: 7s - loss: 1.1228 - accuracy: 0.8244\n",
            "Epoch 4: loss did not improve from 1.10719\n",
            " 28/137 [=====>........................] - ETA: 5s - loss: 1.1202 - accuracy: 0.8251\n",
            "Epoch 4: loss did not improve from 1.10719\n",
            " 38/137 [=======>......................] - ETA: 4s - loss: 1.1110 - accuracy: 0.8267\n",
            "Epoch 4: loss did not improve from 1.10719\n",
            " 48/137 [=========>....................] - ETA: 4s - loss: 1.1059 - accuracy: 0.8269\n",
            "Epoch 4: loss improved from 1.10719 to 1.10549, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0004.ckpt\n",
            " 57/137 [===========>..................] - ETA: 4s - loss: 1.1056 - accuracy: 0.8267\n",
            "Epoch 4: loss did not improve from 1.10549\n",
            " 67/137 [=============>................] - ETA: 4s - loss: 1.1042 - accuracy: 0.8268\n",
            "Epoch 4: loss improved from 1.10549 to 1.10243, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0004.ckpt\n",
            " 77/137 [===============>..............] - ETA: 3s - loss: 1.1033 - accuracy: 0.8272\n",
            "Epoch 4: loss did not improve from 1.10243\n",
            " 87/137 [==================>...........] - ETA: 2s - loss: 1.1016 - accuracy: 0.8275\n",
            "Epoch 4: loss improved from 1.10243 to 1.10209, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0004.ckpt\n",
            " 97/137 [====================>.........] - ETA: 2s - loss: 1.1008 - accuracy: 0.8275\n",
            "Epoch 4: loss improved from 1.10209 to 1.10070, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0004.ckpt\n",
            "107/137 [======================>.......] - ETA: 1s - loss: 1.1034 - accuracy: 0.8272\n",
            "Epoch 4: loss did not improve from 1.10070\n",
            "117/137 [========================>.....] - ETA: 1s - loss: 1.1055 - accuracy: 0.8271\n",
            "Epoch 4: loss did not improve from 1.10070\n",
            "127/137 [==========================>...] - ETA: 0s - loss: 1.1036 - accuracy: 0.8274\n",
            "Epoch 4: loss did not improve from 1.10070\n",
            "137/137 [==============================] - 8s 59ms/step - loss: 1.1037 - accuracy: 0.8274\n",
            "Epoch 5/30\n",
            "  1/137 [..............................] - ETA: 10s - loss: 1.1917 - accuracy: 0.8179\n",
            "Epoch 5: loss did not improve from 1.10070\n",
            " 11/137 [=>............................] - ETA: 6s - loss: 1.0949 - accuracy: 0.8258\n",
            "Epoch 5: loss improved from 1.10070 to 1.08728, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0005.ckpt\n",
            " 21/137 [===>..........................] - ETA: 6s - loss: 1.0618 - accuracy: 0.8302\n",
            "Epoch 5: loss improved from 1.08728 to 1.06072, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0005.ckpt\n",
            " 30/137 [=====>........................] - ETA: 6s - loss: 1.0643 - accuracy: 0.8308\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            " 40/137 [=======>......................] - ETA: 5s - loss: 1.0750 - accuracy: 0.8291\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            " 51/137 [==========>...................] - ETA: 4s - loss: 1.0746 - accuracy: 0.8298\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            " 61/137 [============>.................] - ETA: 4s - loss: 1.0773 - accuracy: 0.8298\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            " 71/137 [==============>...............] - ETA: 3s - loss: 1.0764 - accuracy: 0.8302\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            " 80/137 [================>.............] - ETA: 2s - loss: 1.0784 - accuracy: 0.8299\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            " 90/137 [==================>...........] - ETA: 2s - loss: 1.0736 - accuracy: 0.8306\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            "100/137 [====================>.........] - ETA: 1s - loss: 1.0763 - accuracy: 0.8300\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            "110/137 [=======================>......] - ETA: 1s - loss: 1.0787 - accuracy: 0.8298\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            "121/137 [=========================>....] - ETA: 0s - loss: 1.0784 - accuracy: 0.8298\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            "131/137 [===========================>..] - ETA: 0s - loss: 1.0778 - accuracy: 0.8298\n",
            "Epoch 5: loss did not improve from 1.06072\n",
            "137/137 [==============================] - 7s 49ms/step - loss: 1.0782 - accuracy: 0.8296\n",
            "Epoch 6/30\n",
            "  3/137 [..............................] - ETA: 9s - loss: 1.0998 - accuracy: 0.8267\n",
            "Epoch 6: loss improved from 1.06072 to 1.06040, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0006.ckpt\n",
            " 14/137 [==>...........................] - ETA: 8s - loss: 1.0704 - accuracy: 0.8305\n",
            "Epoch 6: loss did not improve from 1.06040\n",
            " 24/137 [====>.........................] - ETA: 6s - loss: 1.0684 - accuracy: 0.8303\n",
            "Epoch 6: loss did not improve from 1.06040\n",
            " 33/137 [======>.......................] - ETA: 5s - loss: 1.0586 - accuracy: 0.8314\n",
            "Epoch 6: loss improved from 1.06040 to 1.05862, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0006.ckpt\n",
            " 43/137 [========>.....................] - ETA: 5s - loss: 1.0614 - accuracy: 0.8312\n",
            "Epoch 6: loss improved from 1.05862 to 1.05693, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0006.ckpt\n",
            " 53/137 [==========>...................] - ETA: 5s - loss: 1.0621 - accuracy: 0.8312\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            " 63/137 [============>.................] - ETA: 4s - loss: 1.0628 - accuracy: 0.8311\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            " 73/137 [==============>...............] - ETA: 3s - loss: 1.0642 - accuracy: 0.8309\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            " 83/137 [=================>............] - ETA: 2s - loss: 1.0617 - accuracy: 0.8314\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            " 93/137 [===================>..........] - ETA: 2s - loss: 1.0607 - accuracy: 0.8315\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            "103/137 [=====================>........] - ETA: 1s - loss: 1.0574 - accuracy: 0.8320\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            "113/137 [=======================>......] - ETA: 1s - loss: 1.0590 - accuracy: 0.8319\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            "123/137 [=========================>....] - ETA: 0s - loss: 1.0592 - accuracy: 0.8318\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            "133/137 [============================>.] - ETA: 0s - loss: 1.0588 - accuracy: 0.8320\n",
            "Epoch 6: loss did not improve from 1.05693\n",
            "137/137 [==============================] - 7s 50ms/step - loss: 1.0570 - accuracy: 0.8322\n",
            "Epoch 7/30\n",
            "  7/137 [>.............................] - ETA: 7s - loss: 1.0568 - accuracy: 0.8299\n",
            "Epoch 7: loss improved from 1.05693 to 1.04472, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            " 17/137 [==>...........................] - ETA: 7s - loss: 1.0419 - accuracy: 0.8310\n",
            "Epoch 7: loss improved from 1.04472 to 1.03494, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            " 26/137 [====>.........................] - ETA: 7s - loss: 1.0326 - accuracy: 0.8326\n",
            "Epoch 7: loss improved from 1.03494 to 1.03109, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            " 37/137 [=======>......................] - ETA: 7s - loss: 1.0315 - accuracy: 0.8336\n",
            "Epoch 7: loss did not improve from 1.03109\n",
            " 47/137 [=========>....................] - ETA: 5s - loss: 1.0339 - accuracy: 0.8334\n",
            "Epoch 7: loss did not improve from 1.03109\n",
            " 56/137 [===========>..................] - ETA: 4s - loss: 1.0311 - accuracy: 0.8342\n",
            "Epoch 7: loss did not improve from 1.03109\n",
            " 66/137 [=============>................] - ETA: 4s - loss: 1.0280 - accuracy: 0.8348\n",
            "Epoch 7: loss improved from 1.03109 to 1.02921, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            " 76/137 [===============>..............] - ETA: 4s - loss: 1.0305 - accuracy: 0.8349\n",
            "Epoch 7: loss did not improve from 1.02921\n",
            " 86/137 [=================>............] - ETA: 3s - loss: 1.0254 - accuracy: 0.8357\n",
            "Epoch 7: loss improved from 1.02921 to 1.02627, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            " 96/137 [====================>.........] - ETA: 2s - loss: 1.0254 - accuracy: 0.8358\n",
            "Epoch 7: loss improved from 1.02627 to 1.02601, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            "106/137 [======================>.......] - ETA: 2s - loss: 1.0252 - accuracy: 0.8360\n",
            "Epoch 7: loss improved from 1.02601 to 1.02418, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            "116/137 [========================>.....] - ETA: 1s - loss: 1.0245 - accuracy: 0.8360\n",
            "Epoch 7: loss improved from 1.02418 to 1.02375, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0007.ckpt\n",
            "126/137 [==========================>...] - ETA: 0s - loss: 1.0261 - accuracy: 0.8358\n",
            "Epoch 7: loss did not improve from 1.02375\n",
            "137/137 [==============================] - 9s 68ms/step - loss: 1.0232 - accuracy: 0.8363\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: loss improved from 1.02375 to 0.97444, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0008.ckpt\n",
            "  9/137 [>.............................] - ETA: 5s - loss: 0.9903 - accuracy: 0.8388\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 19/137 [===>..........................] - ETA: 5s - loss: 0.9860 - accuracy: 0.8394\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 29/137 [=====>........................] - ETA: 4s - loss: 0.9910 - accuracy: 0.8390\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 39/137 [=======>......................] - ETA: 4s - loss: 0.9877 - accuracy: 0.8396\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 49/137 [=========>....................] - ETA: 3s - loss: 0.9863 - accuracy: 0.8395\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 59/137 [===========>..................] - ETA: 3s - loss: 0.9878 - accuracy: 0.8395\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 69/137 [==============>...............] - ETA: 2s - loss: 0.9845 - accuracy: 0.8400\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 79/137 [================>.............] - ETA: 2s - loss: 0.9839 - accuracy: 0.8400\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 89/137 [==================>...........] - ETA: 2s - loss: 0.9856 - accuracy: 0.8399\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            " 99/137 [====================>.........] - ETA: 1s - loss: 0.9901 - accuracy: 0.8394\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            "109/137 [======================>.......] - ETA: 1s - loss: 0.9941 - accuracy: 0.8389\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            "119/137 [=========================>....] - ETA: 0s - loss: 0.9953 - accuracy: 0.8388\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            "129/137 [===========================>..] - ETA: 0s - loss: 0.9936 - accuracy: 0.8391\n",
            "Epoch 8: loss did not improve from 0.97444\n",
            "137/137 [==============================] - 6s 44ms/step - loss: 0.9944 - accuracy: 0.8389\n",
            "Epoch 9/30\n",
            "  3/137 [..............................] - ETA: 8s - loss: 0.9486 - accuracy: 0.8424\n",
            "Epoch 9: loss improved from 0.97444 to 0.92777, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0009.ckpt\n",
            " 12/137 [=>............................] - ETA: 8s - loss: 0.9288 - accuracy: 0.8449\n",
            "Epoch 9: loss improved from 0.92777 to 0.92478, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0009.ckpt\n",
            " 22/137 [===>..........................] - ETA: 8s - loss: 0.9279 - accuracy: 0.8457\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            " 32/137 [======>.......................] - ETA: 6s - loss: 0.9352 - accuracy: 0.8453\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            " 42/137 [========>.....................] - ETA: 5s - loss: 0.9372 - accuracy: 0.8454\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            " 52/137 [==========>...................] - ETA: 4s - loss: 0.9479 - accuracy: 0.8440\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            " 62/137 [============>.................] - ETA: 3s - loss: 0.9469 - accuracy: 0.8440\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            " 72/137 [==============>...............] - ETA: 3s - loss: 0.9508 - accuracy: 0.8435\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            " 82/137 [================>.............] - ETA: 2s - loss: 0.9527 - accuracy: 0.8434\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            " 92/137 [===================>..........] - ETA: 2s - loss: 0.9543 - accuracy: 0.8431\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            "102/137 [=====================>........] - ETA: 1s - loss: 0.9558 - accuracy: 0.8429\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            "112/137 [=======================>......] - ETA: 1s - loss: 0.9568 - accuracy: 0.8427\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            "122/137 [=========================>....] - ETA: 0s - loss: 0.9568 - accuracy: 0.8428\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            "132/137 [===========================>..] - ETA: 0s - loss: 0.9563 - accuracy: 0.8429\n",
            "Epoch 9: loss did not improve from 0.92478\n",
            "137/137 [==============================] - 7s 48ms/step - loss: 0.9596 - accuracy: 0.8425\n",
            "Epoch 10/30\n",
            "  5/137 [>.............................] - ETA: 5s - loss: 0.9119 - accuracy: 0.8470\n",
            "Epoch 10: loss improved from 0.92478 to 0.90588, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0010.ckpt\n",
            " 15/137 [==>...........................] - ETA: 7s - loss: 0.9153 - accuracy: 0.8440\n",
            "Epoch 10: loss did not improve from 0.90588\n",
            " 25/137 [====>.........................] - ETA: 5s - loss: 0.9022 - accuracy: 0.8465\n",
            "Epoch 10: loss improved from 0.90588 to 0.90335, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0010.ckpt\n",
            " 35/137 [======>.......................] - ETA: 6s - loss: 0.9053 - accuracy: 0.8461\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            " 45/137 [========>.....................] - ETA: 5s - loss: 0.9192 - accuracy: 0.8448\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            " 55/137 [===========>..................] - ETA: 4s - loss: 0.9259 - accuracy: 0.8444\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            " 65/137 [=============>................] - ETA: 3s - loss: 0.9308 - accuracy: 0.8442\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            " 75/137 [===============>..............] - ETA: 3s - loss: 0.9302 - accuracy: 0.8444\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            " 85/137 [=================>............] - ETA: 2s - loss: 0.9307 - accuracy: 0.8447\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            " 95/137 [===================>..........] - ETA: 2s - loss: 0.9309 - accuracy: 0.8445\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            "105/137 [=====================>........] - ETA: 1s - loss: 0.9311 - accuracy: 0.8445\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            "116/137 [========================>.....] - ETA: 1s - loss: 0.9335 - accuracy: 0.8444\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            "126/137 [==========================>...] - ETA: 0s - loss: 0.9349 - accuracy: 0.8441\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            "136/137 [============================>.] - ETA: 0s - loss: 0.9371 - accuracy: 0.8439\n",
            "Epoch 10: loss did not improve from 0.90335\n",
            "137/137 [==============================] - 7s 48ms/step - loss: 0.9376 - accuracy: 0.8439\n",
            "Epoch 11/30\n",
            "  9/137 [>.............................] - ETA: 6s - loss: 0.8973 - accuracy: 0.8450\n",
            "Epoch 11: loss improved from 0.90335 to 0.90078, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0011.ckpt\n",
            " 18/137 [==>...........................] - ETA: 7s - loss: 0.9004 - accuracy: 0.8451\n",
            "Epoch 11: loss improved from 0.90078 to 0.89780, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0011.ckpt\n",
            " 28/137 [=====>........................] - ETA: 9s - loss: 0.8957 - accuracy: 0.8457\n",
            "Epoch 11: loss improved from 0.89780 to 0.89335, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0011.ckpt\n",
            " 38/137 [=======>......................] - ETA: 7s - loss: 0.8899 - accuracy: 0.8467\n",
            "Epoch 11: loss improved from 0.89335 to 0.88603, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0011.ckpt\n",
            " 48/137 [=========>....................] - ETA: 6s - loss: 0.8853 - accuracy: 0.8475\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            " 58/137 [===========>..................] - ETA: 5s - loss: 0.8901 - accuracy: 0.8472\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            " 68/137 [=============>................] - ETA: 4s - loss: 0.8936 - accuracy: 0.8469\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            " 78/137 [================>.............] - ETA: 3s - loss: 0.8968 - accuracy: 0.8466\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            " 88/137 [==================>...........] - ETA: 3s - loss: 0.8959 - accuracy: 0.8467\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            " 98/137 [====================>.........] - ETA: 2s - loss: 0.8955 - accuracy: 0.8470\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            "109/137 [======================>.......] - ETA: 1s - loss: 0.9005 - accuracy: 0.8465\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            "119/137 [=========================>....] - ETA: 1s - loss: 0.9033 - accuracy: 0.8463\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            "129/137 [===========================>..] - ETA: 0s - loss: 0.9049 - accuracy: 0.8462\n",
            "Epoch 11: loss did not improve from 0.88603\n",
            "137/137 [==============================] - 8s 56ms/step - loss: 0.9034 - accuracy: 0.8466\n",
            "Epoch 12/30\n",
            "  1/137 [..............................] - ETA: 7s - loss: 0.9227 - accuracy: 0.8438\n",
            "Epoch 12: loss did not improve from 0.88603\n",
            " 11/137 [=>............................] - ETA: 5s - loss: 0.8734 - accuracy: 0.8468\n",
            "Epoch 12: loss improved from 0.88603 to 0.86882, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0012.ckpt\n",
            " 21/137 [===>..........................] - ETA: 6s - loss: 0.8682 - accuracy: 0.8475\n",
            "Epoch 12: loss did not improve from 0.86882\n",
            " 31/137 [=====>........................] - ETA: 5s - loss: 0.8640 - accuracy: 0.8486\n",
            "Epoch 12: loss did not improve from 0.86882\n",
            " 41/137 [=======>......................] - ETA: 4s - loss: 0.8701 - accuracy: 0.8480\n",
            "Epoch 12: loss improved from 0.86882 to 0.86749, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0012.ckpt\n",
            " 51/137 [==========>...................] - ETA: 4s - loss: 0.8657 - accuracy: 0.8489\n",
            "Epoch 12: loss did not improve from 0.86749\n",
            " 61/137 [============>.................] - ETA: 3s - loss: 0.8685 - accuracy: 0.8487\n",
            "Epoch 12: loss improved from 0.86749 to 0.86739, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0012.ckpt\n",
            " 71/137 [==============>...............] - ETA: 3s - loss: 0.8709 - accuracy: 0.8485\n",
            "Epoch 12: loss did not improve from 0.86739\n",
            " 81/137 [================>.............] - ETA: 3s - loss: 0.8714 - accuracy: 0.8484\n",
            "Epoch 12: loss did not improve from 0.86739\n",
            " 91/137 [==================>...........] - ETA: 2s - loss: 0.8727 - accuracy: 0.8486\n",
            "Epoch 12: loss did not improve from 0.86739\n",
            "102/137 [=====================>........] - ETA: 1s - loss: 0.8734 - accuracy: 0.8485\n",
            "Epoch 12: loss did not improve from 0.86739\n",
            "112/137 [=======================>......] - ETA: 1s - loss: 0.8735 - accuracy: 0.8483\n",
            "Epoch 12: loss did not improve from 0.86739\n",
            "122/137 [=========================>....] - ETA: 0s - loss: 0.8717 - accuracy: 0.8488\n",
            "Epoch 12: loss did not improve from 0.86739\n",
            "132/137 [===========================>..] - ETA: 0s - loss: 0.8703 - accuracy: 0.8491\n",
            "Epoch 12: loss did not improve from 0.86739\n",
            "137/137 [==============================] - 7s 50ms/step - loss: 0.8708 - accuracy: 0.8490\n",
            "Epoch 13/30\n",
            "  5/137 [>.............................] - ETA: 5s - loss: 0.8354 - accuracy: 0.8490\n",
            "Epoch 13: loss improved from 0.86739 to 0.82568, saving model to /content/drive/MyDrive/translate/checkpoint/cp-0013.ckpt\n"
          ]
        }
      ],
      "source": [
        "model.fit(dataset,\n",
        "          epochs=epochs,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGxWYuiQPnv"
      },
      "outputs": [],
      "source": [
        "# if use colab\n",
        "saved_model_path = \"/content/drive/MyDrive/translate/saved_model/translator.h5\"\n",
        "\n",
        "# if use local env\n",
        "# saved_model_path = \"code/translate sentence/saved_model\"\n",
        "saved_model_dir = os.path.dirname(saved_model_path)\n",
        "\n",
        "if os.path.exists(saved_model_dir):\n",
        "  shutil.rmtree(saved_model_dir)\n",
        "model.save(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YmcvYUyzqLd"
      },
      "outputs": [],
      "source": [
        "class Translator:\n",
        "  def __init__(self, model_path, input_tokenizer, target_tokenizer, maxlen):\n",
        "    self.input_tokenizer = input_tokenizer\n",
        "    self.target_tokenizer = target_tokenizer\n",
        "    self.maxlen = maxlen\n",
        "    self.model_path = model_path\n",
        "\n",
        "    self._load_model()\n",
        "\n",
        "  def _load_model(self):\n",
        "    self.model = tf.keras.models.load_model(self.model_path)\n",
        "\n",
        "  def _normalize_and_preprocess(self, text):\n",
        "    punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    \n",
        "    text = text.lower().strip()\n",
        "    text = text.replace(punctuation, \"\")\n",
        "\n",
        "    return text\n",
        "    \n",
        "  def translate(self, sentence):\n",
        "    normalize_sentence = self._normalize_and_preprocess(sentence)\n",
        "\n",
        "    sequences = input_tokenizer.texts_to_sequences([normalize_sentence])\n",
        "    sequences = pad_sequences(sequences, maxlen=self.maxlen, padding=\"post\")\n",
        "    \n",
        "    predictions = self.model.predict(sequences)\n",
        "\n",
        "    index_prediction = list()\n",
        "\n",
        "    for i in predictions[0]:\n",
        "      index_prediction.append(np.argmax(i))\n",
        "\n",
        "    marks = [start_mark, end_mark]\n",
        "    result = target_tokenizer.sequences_to_texts([index_prediction])[0]\n",
        "    result = \" \".join([word for word in result.split(\" \") if word not in marks])\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkNd7WY3LSvJ"
      },
      "outputs": [],
      "source": [
        "translator = Translator(\n",
        "    saved_model_path,\n",
        "    input_tokenizer, \n",
        "    target_tokenizer,\n",
        "    input_maxlen,\n",
        ")\n",
        "\n",
        "translate = translator.translate(\"how are you?\")\n",
        "translate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfjs_path = \"/content/IOH-chat-app/MachineLearning/code/translation/tfjs_model\"\n",
        "\n",
        "!tensorflowjs_converter \\\n",
        "  --input_format=keras \\\n",
        "  {saved_model_path} \\\n",
        "  {tfjs_path}"
      ],
      "metadata": {
        "id": "S4jkk5HO-g6J"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive(\"tfjs_model\", 'zip', tfjs_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "34JcUUHSJ-Wy",
        "outputId": "2742a452-356e-4810-ddfe-4357e0353195"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/tfjs_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"tfjs_model.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3qNmjNO6KrmJ",
        "outputId": "9750a9f8-4c75-4034-eb61-2d4841848353"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_63e5312a-c43d-4f7c-b5e4-2a7d70886cb5\", \"tfjs_model.zip\", 25234885)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "translator.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "765233bfe060b87a8be23ec8f17030d468ac6435ae34b0ad14370b4cb734ac81"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}